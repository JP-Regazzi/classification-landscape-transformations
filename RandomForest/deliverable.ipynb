{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import shapely\n",
    "from shapely.geometry import Polygon, LinearRing\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = ['red', 'green', 'blue']\n",
    "METRICS = ['std', 'mean']\n",
    "\n",
    "CHANGE_TYPE_MAP = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "                   'Mega Projects': 5}\n",
    "CHANGE_STATUS_MAP = {'Greenland': 1, 'Land Cleared': 2, 'Materials Introduced': 3,\n",
    "                     'Prior Construction': 4, 'Excavation': 5, 'Construction Started': 6,\n",
    "                     'Construction Midway': 7, 'Materials Dumped': 8, 'Construction Done': 9,\n",
    "                     'Operational': 10}\n",
    "\n",
    "GEOGRAPHY_TYPES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow', 'N,A']\n",
    "URBAN_TYPES = ['Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial', 'N,A']\n",
    "\n",
    "COLUMNS_TO_DROP = ['geography_type', 'urban_type', 'geometry']\n",
    "DATE_COLUMNS = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "\n",
    "BINARY_FEATURES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow',\n",
    "                   'Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial']\n",
    "CATEGORICAL_FEATURES = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3',\n",
    "                      'change_status_date4']\n",
    "\n",
    "\n",
    "CHANGE_STATUS_VALUE_COLUMNS = ['change_status_value_date0', 'change_status_value_date1', 'change_status_value_date2', 'change_status_value_date3', 'change_status_value_date4']\n",
    "CHANGE_STATUS_COLUMNS = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3', 'change_status_date4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "train_df = gpd.read_file('../data/train.geojson', index_col=0)\n",
    "test_df = gpd.read_file('../data/test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of training data\n",
    "copied_train_df = train_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping strings into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps change_type and change_status values into integers \n",
    "copied_train_df['change_type'] = copied_train_df['change_type'].map(CHANGE_TYPE_MAP)\n",
    "for i in range(5): copied_train_df[f'change_status_date{i}'] = copied_train_df[f'change_status_date{i}'].map(CHANGE_STATUS_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    copied_train_df[\"geography_type\" + geograph_type] = copied_train_df['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "for urban_type in URBAN_TYPES:\n",
    "    copied_train_df[\"urban_type\" + urban_type] = copied_train_df['urban_type'].apply(lambda x: 1 if urban_type in x else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting of date-related features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_date_related_features(row):\n",
    "\n",
    "    # Sort columns by date\n",
    "    columns_order = np.argsort(row[DATE_COLUMNS].values)\n",
    "    new_row = row.copy(deep=True)\n",
    "\n",
    "    # Update date and change_status order\n",
    "    for i in range(5):\n",
    "        new_row[f'date{i}'] = row[f'date{columns_order[i]}']\n",
    "        new_row[f'change_status_date{i}'] = row[f'change_status_date{columns_order[i]}']\n",
    "\n",
    "    # Update color metrics order\n",
    "    for metric in METRICS:\n",
    "        for color in COLORS:\n",
    "            for i in range(1, 6):\n",
    "                new_row[f'img_{color}_{metric}_date{i}'] = row[f'img_{color}_{metric}_date{columns_order[i-1]+1}']\n",
    "\n",
    "    return new_row\n",
    "\n",
    "# Fix data type\n",
    "copied_train_df[DATE_COLUMNS] = copied_train_df[DATE_COLUMNS].apply(lambda x: pd.to_datetime(x, format='%d-%m-%Y', errors='coerce'))\n",
    "\n",
    "# Sort date related features\n",
    "copied_train_df = copied_train_df.apply(sort_date_related_features, axis=1)\n",
    "\n",
    "# Turning date features into float values\n",
    "copied_train_df[DATE_COLUMNS] = copied_train_df[DATE_COLUMNS].apply(np.float64)\n",
    "copied_train_df.loc[:, DATE_COLUMNS] = copied_train_df.loc[:, DATE_COLUMNS].applymap(lambda x: np.nan if x < 0 else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_paralelogram(polygon):\n",
    "    it_is = True\n",
    "    LIMIT = 1e-2\n",
    "    # Get the outer boundary coordinates\n",
    "    coords = polygon.exterior.coords[:-1]  # Exclude the closing coordinate\n",
    "    \n",
    "    # Check if the polygon has four sides\n",
    "    num_vertices = len(coords)\n",
    "    if  num_vertices != 4:  # a paralelogram should have 4 vertices\n",
    "        it_is = False\n",
    "        return (it_is, num_vertices, 10*LIMIT, num_vertices*4*LIMIT)\n",
    "\n",
    "    # Calculate the lengths of the sides\n",
    "    side_lengths = [euclidean(coords[i],coords[(i+1)%4]) for i in range(4)]  # Calculate the length between each pair of adjacent vertices\n",
    "    \n",
    "    # Check if opposite sides have equal length (index 0 and 2, index 1 and 3)\n",
    "    length_dif = abs(side_lengths[0] - side_lengths[2]) + abs(side_lengths[1] - side_lengths[3])/polygon.length\n",
    "    if length_dif > LIMIT:\n",
    "        it_is = False\n",
    "    \n",
    "    return (it_is, num_vertices, length_dif, length_dif)\n",
    "\n",
    "def get_centroid_lat(row):\n",
    "    center = row['geometry'].centroid.xy[1][0]\n",
    "    return center\n",
    "\n",
    "def get_centroid_lon(row):\n",
    "    center = row['geometry'].centroid.xy[0][0]\n",
    "    return center\n",
    "\n",
    "\n",
    "## Create new polygon features\n",
    "\n",
    "copied_train_df['area'] = copied_train_df['geometry'].area\n",
    "copied_train_df['length'] = copied_train_df['geometry'].length\n",
    "copied_train_df['centroid_x'] = copied_train_df['geometry'].centroid.x\n",
    "copied_train_df['centroid_y'] = copied_train_df['geometry'].centroid.y\n",
    "\n",
    "bounds = copied_train_df['geometry'].bounds\n",
    "copied_train_df['angle'] = np.arctan((bounds['maxy']-bounds['miny'])/(bounds['maxx']-bounds['minx']))\n",
    "copied_train_df['compactness'] = 4 * np.pi * (copied_train_df['area'] / copied_train_df['length']**2)\n",
    "\n",
    "tmp = copied_train_df['geometry'].apply(is_paralelogram)\n",
    "copied_train_df['paralelogram'], copied_train_df['num_vertices'], copied_train_df['length_dif1'], copied_train_df['length_dif2'] = zip(*tmp)\n",
    "copied_train_df['rect_area'] = copied_train_df['geometry'].apply(shapely.minimum_rotated_rectangle, axis=1).area\n",
    "\n",
    "convex_hull = copied_train_df['geometry'].convex_hull\n",
    "copied_train_df['dif_convex_prop_area'] = (convex_hull.area - copied_train_df['area'])/ convex_hull.area\n",
    "copied_train_df['convex'] =  copied_train_df['geometry'].geom_equals(convex_hull)\n",
    "\n",
    "# Get latitude and longitude\n",
    "train_df_geo = pd.DataFrame()\n",
    "train_df_geo['Lat'] = copied_train_df.apply(get_centroid_lat, axis=1)\n",
    "train_df_geo['Lon'] = copied_train_df.apply(get_centroid_lon, axis=1)\n",
    "\n",
    "# Use kmeans to cluster latitude and longitude data\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(train_df_geo)\n",
    "labels = kmeans.labels_\n",
    "copied_train_df['geo_cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop of unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_train_df = copied_train_df.drop(COLUMNS_TO_DROP, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Group the DataFrame by \"change_type\"\n",
    "grouped = copied_train_df.groupby(\"change_type\")\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5,missing_values=np.nan)  # You can adjust the number of neighbors as needed\n",
    "\n",
    "# Initialize an empty list to store the imputed DataFrames\n",
    "imputed_dfs = []\n",
    "\n",
    "# Iterate over each group\n",
    "for change_type, group in grouped:\n",
    "    # Drop the \"change_type\" column before imputation\n",
    "    group_features = group.drop(columns=[\"change_type\"])\n",
    "    \n",
    "    # Impute NaN values within the group\n",
    "    imputed_values = imputer.fit_transform(group_features)\n",
    "    \n",
    "    # Create a DataFrame with imputed values\n",
    "    imputed_df = pd.DataFrame(imputed_values, columns=group_features.columns, index=group_features.index)\n",
    "    \n",
    "    # Concatenate \"change_type\" column back to the imputed DataFrame\n",
    "    imputed_df[\"change_type\"] = change_type\n",
    "    \n",
    "    # Append the imputed DataFrame to the list\n",
    "    imputed_dfs.append(imputed_df)\n",
    "\n",
    "# Concatenate all imputed DataFrames into a single DataFrame\n",
    "copied_train_df = pd.concat(imputed_dfs)\n",
    "\n",
    "# Verify if there are still NaN values after imputation\n",
    "if copied_train_df.isnull().values.any():\n",
    "    print(\"There are still NaN values remaining after imputation.\")\n",
    "    print(copied_train_df[copied_train_df.isnull().any(axis=1)])\n",
    "else:\n",
    "    print(\"All NaN values have been imputed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_train_df[CHANGE_STATUS_COLUMNS] = copied_train_df[CHANGE_STATUS_COLUMNS].round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute time and color deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute color delta\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = copied_train_df[f'img_{color}_{metric}_date{i}'] - copied_train_df[f'img_{color}_{metric}_date{i-1}']\n",
    "            copied_train_df[f'img_{color}_{metric}_delta{i}'] = delta\n",
    "        copied_train_df[f'img_{color}_{metric}_delta_total'] = copied_train_df[f'img_{color}_{metric}_date5'] - copied_train_df[f'img_{color}_{metric}_date1']\n",
    "# Compute time delta\n",
    "for i in range(1, 5):\n",
    "    copied_train_df[f'date_delta{i}'] = copied_train_df[f'date{i}'] - copied_train_df[f'date{i-1}']\n",
    "copied_train_df['date_delta_total'] = copied_train_df[f'date4'] - copied_train_df[f'date1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slopes and Rates computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardizing colors mean by the proportion\n",
    "for i in range(1, 6):\n",
    "    color_sum = copied_train_df[f'img_blue_mean_date{i}'] + copied_train_df[f'img_green_mean_date{i}'] + copied_train_df[f'img_red_mean_date{i}']\n",
    "    for color in COLORS:\n",
    "        copied_train_df[f'img_{color}_mean_prop_date{i}'] = copied_train_df[f'img_{color}_mean_date{i}']/color_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create img_{color}_mean_prop_rate\n",
    "num_samples = copied_train_df.shape[0]\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "for color in COLORS:\n",
    "    coef = np.zeros((num_samples))\n",
    "    COLOR_MEAN_COLUMNS = [f'img_{color}_mean_prop_date{i}' for i in range (1,6)]\n",
    "\n",
    "    Y = np.array(copied_train_df[COLOR_MEAN_COLUMNS].astype(float))\n",
    "    X = np.array(copied_train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "    X = np.dstack((ones,X))\n",
    "    nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "    X[nan_mask,:] = 0\n",
    "    Y[nan_mask] = 0\n",
    "\n",
    "    eye = np.eye(2)*0.0001\n",
    "    for i in range(num_samples):\n",
    "        x = X[i].reshape((5,2))\n",
    "        y = Y[i].reshape((5))\n",
    "        coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "\n",
    "    copied_train_df[f'img_{color}_mean_prop_rate'] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create img_{color}_std_rate\n",
    "for color in COLORS:\n",
    "    coef = np.zeros((num_samples))\n",
    "    COLOR_STD_COLUMNS = [f'img_{color}_std_date{i}' for i in range (1,6)]\n",
    "\n",
    "    Y = np.array(copied_train_df[COLOR_STD_COLUMNS].astype(float))\n",
    "    X = np.array(copied_train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "    X = np.dstack((ones,X))\n",
    "    nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "    X[nan_mask,:] = 0\n",
    "    Y[nan_mask] = 0\n",
    "\n",
    "    eye = np.eye(2)*0.0001\n",
    "    for i in range(num_samples):\n",
    "        x = X[i].reshape((5,2))\n",
    "        y = Y[i].reshape((5))\n",
    "        coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "\n",
    "    copied_train_df[f'img_{color}_std_rate'] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create civilization_rate\n",
    "num_samples = copied_train_df.shape[0]\n",
    "coef = np.zeros((num_samples))\n",
    "time_ctt = 1e9*60*90*24\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "Y = np.array(copied_train_df[CHANGE_STATUS_COLUMNS].astype(float))\n",
    "Y_nan_mask = np.isnan(Y)\n",
    "X = np.array(copied_train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]/time_ctt\n",
    "X = np.dstack((ones,X))\n",
    "X[Y_nan_mask,:] = 0\n",
    "Y[Y_nan_mask] = 0\n",
    "\n",
    "eye = np.eye(2)*0.0001\n",
    "for i in range(num_samples):\n",
    "    x = X[i].reshape((5,2))\n",
    "    y = Y[i].reshape((5))\n",
    "    coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "    #print(y, train_df[\"change_type\"].iloc[i])\n",
    "copied_train_df[\"civilizating_rate\"] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_to_train_df = copied_train_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our architecture utilizes three models to do the prediction. A brief summary is presented below:\n",
    "-   We train a first random forest to predict whether change type is in (1, 2, 3) or (4, 5)\n",
    "-   We train a second random forest to separate classes inside group (4, 5) to 4 or 5\n",
    "-   We train a third random forest to separate classes inside group (1, 2, 3) to 1 or 2 or 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create groups (1, 2 ,3) and (4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_to_train_df[\"industrial_and_megaprojects\"] = (ready_to_train_df[\"change_type\"] > 3).replace({True: 1, False: 0})\n",
    "indus_mega_ready_to_train_df = ready_to_train_df.loc[ready_to_train_df['industrial_and_megaprojects'] == 1]\n",
    "others_ready_to_train_df = ready_to_train_df.loc[ready_to_train_df['industrial_and_megaprojects'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling of most common target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=142)\n",
    "X = ready_to_train_df.loc[:, ready_to_train_df.columns != \"industrial_and_megaprojects\"]\n",
    "y = ready_to_train_df[\"industrial_and_megaprojects\"]\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids(random_state=142)\n",
    "X = ready_to_train_df.loc[:, ready_to_train_df.columns != \"industrial_and_megaprojects\"]\n",
    "y = ready_to_train_df[\"industrial_and_megaprojects\"]\n",
    "X_resampled, y_resampled = cc.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concated = pd.concat([X_resampled, y_resampled], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round numerical values to reduce computational burden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_round = ['change_status_date0',\n",
    " 'change_status_date1',\n",
    " 'change_status_date2',\n",
    " 'change_status_date3',\n",
    " 'change_status_date4','geography_typeDense Forest',\n",
    " 'geography_typeGrass Land',\n",
    " 'geography_typeSparse Forest',\n",
    " 'geography_typeFarms',\n",
    " 'geography_typeRiver',\n",
    " 'geography_typeCoastal',\n",
    " 'geography_typeLakes',\n",
    " 'geography_typeBarren Land',\n",
    " 'geography_typeDesert',\n",
    " 'geography_typeHills',\n",
    " 'geography_typeSnow',\n",
    " 'geography_typeN,A',\n",
    " 'urban_typeSparse Urban',\n",
    " 'urban_typeRural',\n",
    " 'urban_typeDense Urban',\n",
    " 'urban_typeUrban Slum',\n",
    " 'urban_typeIndustrial',\n",
    " 'urban_typeN,A',\n",
    "]\n",
    "train_concated[features_to_round] = train_concated[features_to_round].round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train first tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tree_features =  [\n",
    "'length','centroid_y', 'centroid_x', 'civilizating_rate', 'geography_typeFarms',\n",
    "'geography_typeCoastal', 'geography_typeLakes', 'urban_typeUrban Slum', 'urban_typeIndustrial',\n",
    "'date0', 'img_red_mean_prop_rate', 'img_green_mean_prop_rate', 'img_blue_mean_prop_rate',\n",
    "'area', 'date_delta_total', 'change_status_date4', 'date1', 'compactness', 'img_red_std_rate', 'paralelogram',\n",
    "'num_vertices', 'length_dif1', 'length_dif2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset and fit first model\n",
    "\n",
    "y = ready_to_train_df[\"industrial_and_megaprojects\"]\n",
    "X = ready_to_train_df[first_tree_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "clf_first_tree = RandomForestClassifier(n_estimators=50, random_state=42, min_samples_leaf=1)\n",
    "clf_first_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "y_predicted_train = clf_first_tree.predict(X_train)\n",
    "y_predicted_real = clf_first_tree.predict(ready_to_train_df[first_tree_features])\n",
    "y_predicted_test = clf_first_tree.predict(X_test)\n",
    "\n",
    "print('Train:')\n",
    "print(precision_score(y_train, y_predicted_train, average=\"micro\"))\n",
    "print(f1_score(y_train, y_predicted_train, average=\"macro\"))\n",
    "print('Real:')\n",
    "print(precision_score(ready_to_train_df[\"industrial_and_megaprojects\"], y_predicted_real, average=\"micro\"))\n",
    "print(f1_score(ready_to_train_df[\"industrial_and_megaprojects\"], y_predicted_real, average=\"macro\"))\n",
    "print('Test:')\n",
    "print(precision_score(y_test, y_predicted_test, average=\"micro\"))\n",
    "print(f1_score(y_test, y_predicted_test, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train second tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tree_features =  [\n",
    "'length','centroid_y', 'centroid_x', 'civilizating_rate', 'geography_typeFarms',\n",
    "'geography_typeCoastal', 'geography_typeLakes', 'urban_typeUrban Slum', 'urban_typeIndustrial',\n",
    "'date0', 'img_red_mean_prop_rate', 'img_green_mean_prop_rate', 'img_blue_mean_prop_rate',\n",
    "'area', 'date_delta_total', 'change_status_date4', 'date1', 'compactness', 'img_red_std_rate', 'paralelogram',\n",
    "'num_vertices', 'length_dif1', 'length_dif2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset and fit second model \n",
    "y = indus_mega_ready_to_train_df[\"change_type\"]\n",
    "X = indus_mega_ready_to_train_df[second_tree_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "clf_second_tree = RandomForestClassifier(n_estimators=50, random_state=42, min_samples_leaf=1)\n",
    "clf_second_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "y_predicted_train = clf_second_tree.predict(X_train)\n",
    "y_predicted_test = clf_second_tree.predict(X_test)\n",
    "\n",
    "print('Train:')\n",
    "print(precision_score(y_train, y_predicted_train, average=\"micro\"))\n",
    "print(f1_score(y_train, y_predicted_train, average=\"macro\"))\n",
    "print('Test:')\n",
    "print(precision_score(y_test, y_predicted_test, average=\"micro\"))\n",
    "print(f1_score(y_test, y_predicted_test, average=\"macro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_tree_features =  [\n",
    "'length','centroid_y', 'centroid_x', 'civilizating_rate', 'geography_typeFarms',\n",
    "'geography_typeCoastal', 'geography_typeLakes', 'urban_typeUrban Slum', 'urban_typeIndustrial',\n",
    "'date0', 'img_red_mean_prop_rate', 'img_green_mean_prop_rate', 'img_blue_mean_prop_rate',\n",
    "'area', 'date_delta_total', 'change_status_date4', 'date1', 'compactness', 'img_red_std_rate', 'paralelogram',\n",
    "'num_vertices', 'length_dif1', 'length_dif2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset and fit third model \n",
    "y = others_ready_to_train_df[\"change_type\"]\n",
    "X = others_ready_to_train_df[third_tree_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "clf_third_tree = RandomForestClassifier(n_estimators=100, random_state=42, min_samples_leaf=1)\n",
    "clf_third_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "y_predicted_train = clf_third_tree.predict(X_train)\n",
    "y_predicted_test = clf_third_tree.predict(X_test)\n",
    "\n",
    "print('Train:')\n",
    "print(precision_score(y_train, y_predicted_train, average=\"micro\"))\n",
    "print(f1_score(y_train, y_predicted_train, average=\"macro\"))\n",
    "print('Test:')\n",
    "print(precision_score(y_test, y_predicted_test, average=\"micro\"))\n",
    "print(f1_score(y_test, y_predicted_test, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply same transformations made in train_df to test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of test data\n",
    "copied_test_df = test_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps change_type and change_status values into integers \n",
    "for i in range(5): copied_test_df[f'change_status_date{i}'] = copied_test_df[f'change_status_date{i}'].map(CHANGE_STATUS_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    copied_test_df[\"geography_type\" + geograph_type] = copied_test_df['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "for urban_type in URBAN_TYPES:\n",
    "    copied_test_df[\"urban_type\" + urban_type] = copied_test_df['urban_type'].apply(lambda x: 1 if urban_type in x else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data type\n",
    "copied_test_df[DATE_COLUMNS] = copied_test_df[DATE_COLUMNS].apply(lambda x: pd.to_datetime(x, format='%d-%m-%Y', errors='coerce'))\n",
    "\n",
    "# Sort date related features\n",
    "copied_test_df = copied_test_df.apply(sort_date_related_features, axis=1)\n",
    "\n",
    "# Turning date features into float values\n",
    "copied_test_df[DATE_COLUMNS] = copied_test_df[DATE_COLUMNS].apply(np.float64)\n",
    "copied_test_df.loc[:, DATE_COLUMNS] = copied_test_df.loc[:, DATE_COLUMNS].applymap(lambda x: np.nan if x < 0 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new polygon features\n",
    "\n",
    "copied_test_df['area'] = copied_test_df['geometry'].area\n",
    "copied_test_df['length'] = copied_test_df['geometry'].length\n",
    "copied_test_df['centroid_x'] = copied_test_df['geometry'].centroid.x\n",
    "copied_test_df['centroid_y'] = copied_test_df['geometry'].centroid.y\n",
    "\n",
    "bounds = copied_test_df['geometry'].bounds\n",
    "copied_test_df['angle'] = np.arctan((bounds['maxy']-bounds['miny'])/(bounds['maxx']-bounds['minx']))\n",
    "copied_test_df['compactness'] = 4 * np.pi * (copied_test_df['area'] / copied_test_df['length']**2)\n",
    "\n",
    "tmp = copied_test_df['geometry'].apply(is_paralelogram)\n",
    "copied_test_df['paralelogram'], copied_test_df['num_vertices'], copied_test_df['length_dif1'], copied_test_df['length_dif2'] = zip(*tmp)\n",
    "copied_test_df['rect_area'] = copied_test_df['geometry'].apply(shapely.minimum_rotated_rectangle, axis=1).area\n",
    "\n",
    "convex_hull = copied_test_df['geometry'].convex_hull\n",
    "copied_test_df['dif_convex_prop_area'] = (convex_hull.area - copied_test_df['area'])/ convex_hull.area\n",
    "copied_test_df['convex'] =  copied_test_df['geometry'].geom_equals(convex_hull)\n",
    "\n",
    "# Get latitude and longitude\n",
    "test_df_geo = pd.DataFrame()\n",
    "test_df_geo['Lat'] = copied_test_df.apply(get_centroid_lat, axis=1)\n",
    "test_df_geo['Lon'] = copied_test_df.apply(get_centroid_lon, axis=1)\n",
    "\n",
    "# Use kmeans to cluster latitude and longitude data\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(test_df_geo)\n",
    "labels = kmeans.labels_\n",
    "copied_test_df['geo_cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns\n",
    "copied_test_df = copied_test_df.drop(COLUMNS_TO_DROP, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Impute missing data ###\n",
    "\n",
    "# Group the DataFrame by \"change_type\"\n",
    "grouped = copied_test_df.groupby(\"change_type\")\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5,missing_values=np.nan)  # You can adjust the number of neighbors as needed\n",
    "\n",
    "# Initialize an empty list to store the imputed DataFrames\n",
    "imputed_dfs = []\n",
    "\n",
    "# Iterate over each group\n",
    "for change_type, group in grouped:\n",
    "    # Drop the \"change_type\" column before imputation\n",
    "    group_features = group.drop(columns=[\"change_type\"])\n",
    "    \n",
    "    # Impute NaN values within the group\n",
    "    imputed_values = imputer.fit_transform(group_features)\n",
    "    \n",
    "    # Create a DataFrame with imputed values\n",
    "    imputed_df = pd.DataFrame(imputed_values, columns=group_features.columns, index=group_features.index)\n",
    "    \n",
    "    # Concatenate \"change_type\" column back to the imputed DataFrame\n",
    "    imputed_df[\"change_type\"] = change_type\n",
    "    \n",
    "    # Append the imputed DataFrame to the list\n",
    "    imputed_dfs.append(imputed_df)\n",
    "\n",
    "# Concatenate all imputed DataFrames into a single DataFrame\n",
    "copied_test_df = pd.concat(imputed_dfs)\n",
    "\n",
    "# Verify if there are still NaN values after imputation\n",
    "if copied_test_df.isnull().values.any():\n",
    "    print(\"There are still NaN values remaining after imputation.\")\n",
    "    print(copied_test_df[copied_test_df.isnull().any(axis=1)])\n",
    "else:\n",
    "    print(\"All NaN values have been imputed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_test_df[CHANGE_STATUS_COLUMNS] = copied_test_df[CHANGE_STATUS_COLUMNS].round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute color delta\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = copied_test_df[f'img_{color}_{metric}_date{i}'] - copied_test_df[f'img_{color}_{metric}_date{i-1}']\n",
    "            copied_test_df[f'img_{color}_{metric}_delta{i}'] = delta\n",
    "        copied_test_df[f'img_{color}_{metric}_delta_total'] = copied_test_df[f'img_{color}_{metric}_date5'] - copied_test_df[f'img_{color}_{metric}_date1']\n",
    "# Compute time delta\n",
    "for i in range(1, 5):\n",
    "    copied_test_df[f'date_delta{i}'] = copied_test_df[f'date{i}'] - copied_test_df[f'date{i-1}']\n",
    "copied_test_df['date_delta_total'] = copied_test_df[f'date4'] - copied_test_df[f'date1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardizing colors mean by the proportion\n",
    "for i in range(1, 6):\n",
    "    color_sum = copied_test_df[f'img_blue_mean_date{i}'] + copied_test_df[f'img_green_mean_date{i}'] + copied_test_df[f'img_red_mean_date{i}']\n",
    "    for color in COLORS:\n",
    "        copied_test_df[f'img_{color}_mean_prop_date{i}'] = copied_test_df[f'img_{color}_mean_date{i}']/color_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create img_{color}_mean_prop_rate\n",
    "num_samples = copied_test_df.shape[0]\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "for color in COLORS:\n",
    "    coef = np.zeros((num_samples))\n",
    "    COLOR_MEAN_COLUMNS = [f'img_{color}_mean_prop_date{i}' for i in range (1,6)]\n",
    "\n",
    "    Y = np.array(copied_test_df[COLOR_MEAN_COLUMNS].astype(float))\n",
    "    X = np.array(copied_test_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "    X = np.dstack((ones,X))\n",
    "    nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "    X[nan_mask,:] = 0\n",
    "    Y[nan_mask] = 0\n",
    "\n",
    "    eye = np.eye(2)*0.0001\n",
    "    for i in range(num_samples):\n",
    "        x = X[i].reshape((5,2))\n",
    "        y = Y[i].reshape((5))\n",
    "        coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "\n",
    "    copied_test_df[f'img_{color}_mean_prop_rate'] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create img_{color}_std_rate\n",
    "for color in COLORS:\n",
    "    coef = np.zeros((num_samples))\n",
    "    COLOR_STD_COLUMNS = [f'img_{color}_std_date{i}' for i in range (1,6)]\n",
    "\n",
    "    Y = np.array(copied_test_df[COLOR_STD_COLUMNS].astype(float))\n",
    "    X = np.array(copied_test_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "    X = np.dstack((ones,X))\n",
    "    nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "    X[nan_mask,:] = 0\n",
    "    Y[nan_mask] = 0\n",
    "\n",
    "    eye = np.eye(2)*0.0001\n",
    "    for i in range(num_samples):\n",
    "        x = X[i].reshape((5,2))\n",
    "        y = Y[i].reshape((5))\n",
    "        coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "\n",
    "    copied_test_df[f'img_{color}_std_rate'] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create civilization_rate\n",
    "num_samples = copied_test_df.shape[0]\n",
    "coef = np.zeros((num_samples))\n",
    "time_ctt = 1e9*60*90*24\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "Y = np.array(copied_test_df[CHANGE_STATUS_COLUMNS].astype(float))\n",
    "Y_nan_mask = np.isnan(Y)\n",
    "X = np.array(copied_test_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]/time_ctt\n",
    "X = np.dstack((ones,X))\n",
    "X[Y_nan_mask,:] = 0\n",
    "Y[Y_nan_mask] = 0\n",
    "\n",
    "eye = np.eye(2)*0.0001\n",
    "for i in range(num_samples):\n",
    "    x = X[i].reshape((5,2))\n",
    "    y = Y[i].reshape((5))\n",
    "    coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "copied_test_df[\"civilizating_rate\"] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_to_test_df = copied_test_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted1 = clf_first_tree.predict(ready_to_test_df[first_tree_features])\n",
    "y_test_predicted2 = clf_second_tree.predict(ready_to_test_df[second_tree_features])\n",
    "y_test_predicted3 = clf_third_tree.predict(ready_to_test_df[third_tree_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_result(row):\n",
    "   if row[0] == 0:\n",
    "      return row[2]\n",
    "   if row[0] == 1:\n",
    "      return row[1]\n",
    "\n",
    "options = pd.DataFrame(np.array([y_test_predicted1, y_test_predicted2, y_test_predicted3]).T)   \n",
    "options[\"change_type\"] = options.apply(final_result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options[\"change_type\"].to_csv(\"to_submit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
