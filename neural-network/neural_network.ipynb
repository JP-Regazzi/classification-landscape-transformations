{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow import argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Mapping\n",
    "CHANGE_TYPE_MAP = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "                   'Mega Projects': 5}\n",
    "CHANGE_STATUS_MAP = {None: None, 'Greenland': 1, 'Land Cleared': 2, 'Materials Introduced': 3,\n",
    "                     'Prior Construction': 4, 'Excavation': 5, 'Construction Started': 6,\n",
    "                     'Construction Midway': 7, 'Materials Dumped': 8, 'Construction Done': 9,\n",
    "                     'Operational': 10}\n",
    "\n",
    "# Data\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "METRICS = ['std', 'mean']\n",
    "GEOGRAPHY_TYPES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow'] \n",
    "URBAN_TYPES = ['Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial']\n",
    "\n",
    "# Column groups\n",
    "COLUMNS_TO_DROP = ['geography_type', 'urban_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4']\n",
    "DATE_COLUMNS = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "\n",
    "# Feature types\n",
    "BINARY_FEATURES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow',\n",
    "                   'Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial'] \n",
    "CATEGORICAL_FEATURES = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3',\n",
    "                      'change_status_date4']\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = 'preprocessed_train.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "original_train_df = gpd.read_file(f'{BASE_PATH}/data/train.geojson', index_col=0)\n",
    "#test_df = gpd.read_file(f'{BASE_PATH}/data/test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "train_df = original_train_df.copy(deep=True)\n",
    "\n",
    "# Apply Mapping\n",
    "train_df['change_type'] = train_df['change_type'].map(CHANGE_TYPE_MAP)\n",
    "for i in range(5): train_df[f'change_status_date{i}'] = train_df[f'change_status_date{i}'].map(CHANGE_STATUS_MAP)\n",
    "\n",
    "# Fill missing img data with 0\n",
    "train_df = train_df.fillna({col: 0 if 'img_' in col else np.nan for col in train_df.columns})\n",
    "\n",
    "# Change date type\n",
    "train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(lambda x: pd.to_datetime(x, format='%d-%m-%Y', errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dates(row):\n",
    "\n",
    "    # Sort columns by date\n",
    "    columns_order = np.argsort(row[DATE_COLUMNS].values)\n",
    "    new_row = row.copy(deep=True)\n",
    "\n",
    "    # Update date and change_status order\n",
    "    for i in range(5):\n",
    "        new_row[f'date{i}'] = row[f'date{columns_order[i]}']\n",
    "        new_row[f'change_status_date{i}'] = row[f'change_status_date{columns_order[i]}']\n",
    "\n",
    "    # Update color metrics order\n",
    "    for metric in METRICS:\n",
    "        for color in COLORS:\n",
    "            for i in range(1, 6):\n",
    "                new_row[f'img_{color}_{metric}_date{i}'] = row[f'img_{color}_{metric}_date{columns_order[i-1]+1}']\n",
    "    \n",
    "    return new_row\n",
    "\n",
    "train_df = train_df.apply(sort_dates, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    train_df[geograph_type] = train_df['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "for urban_type in URBAN_TYPES:\n",
    "    train_df[urban_type] = train_df['urban_type'].apply(lambda x: 1 if urban_type in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joao Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\shapely\\measurement.py:44: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "c:\\Users\\Joao Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\shapely\\measurement.py:182: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "c:\\Users\\Joao Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\shapely\\constructive.py:285: RuntimeWarning: invalid value encountered in centroid\n",
      "  return lib.centroid(geometry, **kwargs)\n",
      "c:\\Users\\Joao Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\shapely\\constructive.py:285: RuntimeWarning: invalid value encountered in centroid\n",
      "  return lib.centroid(geometry, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create new polygon features\n",
    "train_df = train_df.to_crs('EPSG:3857')\n",
    "train_df['area'] = train_df['geometry'].area\n",
    "train_df['length'] = train_df['geometry'].length\n",
    "train_df['centroid_x'] = train_df['geometry'].centroid.x\n",
    "train_df['centroid_y'] = train_df['geometry'].centroid.y\n",
    "\n",
    "# Create new date related features\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = train_df[f'img_{color}_{metric}_date{i}'] - train_df[f'img_{color}_{metric}_date{i-1}']\n",
    "            train_df[f'img_{color}_{metric}_delta{i}'] = delta\n",
    "        train_df[f'img_{color}_{metric}_delta_total'] = train_df[f'img_{color}_{metric}_date5'] - train_df[f'img_{color}_{metric}_date1']\n",
    "\n",
    "for i in range(1, 5):\n",
    "    date_delta = (train_df[f'date{i}'] - train_df[f'date{i-1}']).dt.days\n",
    "    train_df[f'date_delta{i}'] = date_delta.apply(lambda value: int(value) if pd.notna(value) else np.nan)\n",
    "\n",
    "date_delta_total = (train_df[f'date0'] - train_df[f'date4']).dt.days\n",
    "train_df[f'date_delta_total'] = date_delta_total.apply(lambda value: int(value) if pd.notna(value) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix date\n",
    "#train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(lambda x: pd.to_datetime(x, format='%d-%m-%Y', errors='coerce'))\n",
    "time_ctt = 1e9 * 60 * 90 * 24\n",
    "\n",
    "def fit_and_predict(row):\n",
    "\n",
    "    if row.isna().any():\n",
    "        return np.nan\n",
    "    \n",
    "    x_sample = row[DATE_COLUMNS].apply(lambda x: x.timestamp()).astype(np.float64) / time_ctt\n",
    "    y_sample = row.filter(regex=r'^change_status_date\\d$')\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(x_sample.values.reshape(-1, 1), y_sample.values.reshape(-1, 1))\n",
    "    \n",
    "    return model.coef_[0, 0]\n",
    "\n",
    "train_df[\"civilizating_rate\"] = train_df.apply(fit_and_predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop uncessary columns\n",
    "train_df = train_df.drop(columns=COLUMNS_TO_DROP).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization of numeric features\n",
    "# MUDAR PARA SO FAZER NO TREINO (N STANDARDIZAR O DF INTEIRO PQ PARTE VAI PRA TESTE)\n",
    "numeric_features = [col for col in train_df.columns if col not in BINARY_FEATURES + CATEGORICAL_FEATURES]\n",
    "numeric_features.remove('change_type')\n",
    "for col_name in numeric_features:\n",
    "    mean_value = train_df[col_name].mean()\n",
    "    std_value = train_df[col_name].std()\n",
    "    train_df[col_name] = (train_df[col_name] - mean_value) / std_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date0',\n",
       " 'change_status_date0',\n",
       " 'date1',\n",
       " 'change_status_date1',\n",
       " 'date2',\n",
       " 'change_status_date2',\n",
       " 'date3',\n",
       " 'change_status_date3',\n",
       " 'date4',\n",
       " 'change_status_date4']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns[train_df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = np.array(train_df.drop('change_type', axis=1))\n",
    "y = np.array(train_df['change_type'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6630/6630 - 27s - loss: 0.9464 - accuracy: 0.5966 - val_loss: 0.8263 - val_accuracy: 0.6595 - 27s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "6630/6630 - 25s - loss: 0.8645 - accuracy: 0.6413 - val_loss: 0.7941 - val_accuracy: 0.6699 - 25s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "6630/6630 - 24s - loss: 0.8465 - accuracy: 0.6489 - val_loss: 0.7985 - val_accuracy: 0.6716 - 24s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "6630/6630 - 24s - loss: 0.8367 - accuracy: 0.6529 - val_loss: 0.7853 - val_accuracy: 0.6808 - 24s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "6630/6630 - 22s - loss: 0.8309 - accuracy: 0.6565 - val_loss: 0.7812 - val_accuracy: 0.6755 - 22s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "6630/6630 - 24s - loss: 0.8256 - accuracy: 0.6578 - val_loss: 0.7815 - val_accuracy: 0.6727 - 24s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "6630/6630 - 24s - loss: 0.8246 - accuracy: 0.6597 - val_loss: 0.7770 - val_accuracy: 0.6746 - 24s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "6630/6630 - 24s - loss: 0.8220 - accuracy: 0.6607 - val_loss: 0.7753 - val_accuracy: 0.6837 - 24s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "6630/6630 - 23s - loss: 0.8178 - accuracy: 0.6632 - val_loss: 0.7699 - val_accuracy: 0.6811 - 23s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "6630/6630 - 24s - loss: 0.8180 - accuracy: 0.6636 - val_loss: 0.7694 - val_accuracy: 0.6788 - 24s/epoch - 4ms/step\n",
      "1842/1842 [==============================] - 3s 2ms/step - loss: 0.7720 - accuracy: 0.6823\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(516, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Original accuracy without feature selection\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=2)\n",
    "_, original_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = argmax(y_pred_probs, axis=1)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print('F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network with tree-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Select features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m importances_rf \u001b[38;5;241m=\u001b[39m model_rf\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m----> 7\u001b[0m selected_features_rf \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m[importances_rf \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.05\u001b[39m] \u001b[38;5;66;03m#ADJUST AS NEEDED\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train model and evaluate accuracy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_rf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchange_type\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[selected_features_rf])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Train random forest\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X, y)\n",
    "\n",
    "# Select features\n",
    "importances_rf = model_rf.feature_importances_\n",
    "selected_features_rf = train_df.columns[importances_rf > 0.05] #ADJUST AS NEEDED\n",
    "\n",
    "# Train model and evaluate accuracy\n",
    "X_rf = np.array(train_df.drop('change_type', axis=1)[selected_features_rf])\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_rf, y_train_rf, epochs=10, batch_size=32, validation_split=0.1, verbose=2)\n",
    "accuracy_rf = model.evaluate(X_rf, y_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04897911e-06, 1.22045434e-04, 3.20515363e-04, 4.03005988e-04,\n",
       "       4.69997253e-04, 7.30753138e-04, 1.07559690e-03, 1.23989401e-03,\n",
       "       1.26465342e-03, 1.30604251e-03, 1.38456977e-03, 1.45127151e-03,\n",
       "       1.52459177e-03, 1.52885532e-03, 2.23257786e-03, 5.37559599e-03,\n",
       "       8.98361593e-03, 9.09044365e-03, 9.10536103e-03, 9.18390725e-03,\n",
       "       9.18705059e-03, 9.18857899e-03, 9.20033518e-03, 9.24084851e-03,\n",
       "       9.24334547e-03, 9.26423038e-03, 9.26653853e-03, 9.35583821e-03,\n",
       "       9.37804865e-03, 9.48830878e-03, 9.49003235e-03, 9.49054055e-03,\n",
       "       9.49970214e-03, 9.50261245e-03, 9.50400438e-03, 9.52414735e-03,\n",
       "       9.53587806e-03, 9.53950973e-03, 9.67400214e-03, 9.67474122e-03,\n",
       "       9.70160209e-03, 9.72037811e-03, 9.72490179e-03, 9.72523285e-03,\n",
       "       9.79050794e-03, 9.83345891e-03, 9.86182764e-03, 9.91448667e-03,\n",
       "       9.92761394e-03, 9.94983621e-03, 9.96209902e-03, 9.97558526e-03,\n",
       "       1.00212124e-02, 1.00856644e-02, 1.01016046e-02, 1.01281580e-02,\n",
       "       1.01837357e-02, 1.01841648e-02, 1.01900371e-02, 1.01969143e-02,\n",
       "       1.03130340e-02, 1.03200408e-02, 1.03905693e-02, 1.04343701e-02,\n",
       "       1.05895470e-02, 1.07309257e-02, 1.07632144e-02, 1.10296509e-02,\n",
       "       1.11752619e-02, 1.13267800e-02, 1.15845409e-02, 1.18017817e-02,\n",
       "       1.23794067e-02, 1.24994136e-02, 1.26119913e-02, 1.32660639e-02,\n",
       "       1.43980266e-02, 1.44250032e-02, 1.47549936e-02, 1.52570297e-02,\n",
       "       1.55642543e-02, 1.82302273e-02, 1.87588717e-02, 1.96578495e-02,\n",
       "       2.17806478e-02, 2.18934909e-02, 2.25195868e-02, 3.23819617e-02,\n",
       "       3.97216637e-02, 4.76133604e-02, 5.76047821e-02])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate PCA\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mPCA\u001b[49m(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m) \u001b[38;5;66;03m# ADJUST\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define features and target variable\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate PCA\n",
    "pca = PCA(n_components=0.8) # ADJUST\n",
    "\n",
    "# Define features and target variable\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and compile model\n",
    "pca_model = keras.Sequential([\n",
    "    keras.layers.Dense(512, activation='relu', input_shape=(X_train_pca.shape[1],)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(128, activation='relu'),  # Additional layer\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "pca_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model and evaluate accuracy\n",
    "pca_model.fit(X_train_pca, y_train_pca, epochs=20, batch_size=32, validation_split=0.1, verbose=2)\n",
    "_, original_accuracy = pca_model.evaluate(X_test_pca, y_test_pca)\n",
    "\n",
    "# Calculate F1 score\n",
    "y_pred_probs = pca_model.predict(X_test_pca)\n",
    "y_pred = argmax(y_pred_probs, axis=1)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print('F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network with SelectKBest feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6630/6630 - 21s - loss: 1.0146 - accuracy: 0.5513 - val_loss: 0.9094 - val_accuracy: 0.6013 - 21s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "6630/6630 - 19s - loss: 0.9382 - accuracy: 0.5919 - val_loss: 0.8813 - val_accuracy: 0.6308 - 19s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "6630/6630 - 19s - loss: 0.9165 - accuracy: 0.6030 - val_loss: 0.8508 - val_accuracy: 0.6417 - 19s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "6630/6630 - 19s - loss: 0.9033 - accuracy: 0.6111 - val_loss: 0.8504 - val_accuracy: 0.6416 - 19s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "6630/6630 - 19s - loss: 0.8955 - accuracy: 0.6170 - val_loss: 0.8437 - val_accuracy: 0.6487 - 19s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "6630/6630 - 19s - loss: 0.8914 - accuracy: 0.6199 - val_loss: 0.8310 - val_accuracy: 0.6541 - 19s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "6630/6630 - 19s - loss: 0.8863 - accuracy: 0.6229 - val_loss: 0.8244 - val_accuracy: 0.6534 - 19s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "6630/6630 - 21s - loss: 0.8831 - accuracy: 0.6240 - val_loss: 0.8228 - val_accuracy: 0.6615 - 21s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "6630/6630 - 22s - loss: 0.8804 - accuracy: 0.6263 - val_loss: 0.8329 - val_accuracy: 0.6489 - 22s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "6630/6630 - 22s - loss: 0.8794 - accuracy: 0.6264 - val_loss: 0.8338 - val_accuracy: 0.6592 - 22s/epoch - 3ms/step\n",
      "1842/1842 [==============================] - 3s 2ms/step - loss: 0.8375 - accuracy: 0.6586\n",
      "1842/1842 [==============================] - 3s 1ms/step\n",
      "F1 Score:  0.6417822014769109\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_kbest = scaler.fit_transform(X_train)\n",
    "X_test_kbest = scaler.transform(X_test)\n",
    "\n",
    "# Perform feature selection using SelectKBest and chi-squared\n",
    "k_best = SelectKBest(score_func=chi2, k=80)\n",
    "X_train_kbest = k_best.fit_transform(X_train_kbest, y_train)\n",
    "X_test_kbest = k_best.transform(X_test_kbest)\n",
    "\n",
    "# Create and compile model\n",
    "kbest_model = keras.Sequential([\n",
    "    keras.layers.Dense(512, activation='relu', input_shape=(X_train_kbest.shape[1],)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "kbest_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model and evaluate accuracy\n",
    "kbest_model.fit(X_train_kbest, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=2)\n",
    "_, kbest_accuracy = kbest_model.evaluate(X_test_kbest, y_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "y_pred_probs = kbest_model.predict(X_test_kbest)\n",
    "y_pred = argmax(y_pred_probs, axis=1)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print('F1 Score: ', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
