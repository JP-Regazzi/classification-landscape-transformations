{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Mapping\n",
    "CHANGE_TYPE_MAP = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "                   'Mega Projects': 5}\n",
    "CHANGE_STATUS_MAP = {None: 0, 'Greenland': 1, 'Land Cleared': 2, 'Materials Introduced': 3,\n",
    "                     'Prior Construction': 4, 'Excavation': 5, 'Construction Started': 6,\n",
    "                     'Construction Midway': 7, 'Materials Dumped': 8, 'Construction Done': 9,\n",
    "                     'Operational': 10}\n",
    "\n",
    "# Data\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "METRICS = ['std', 'mean']\n",
    "GEOGRAPHY_TYPES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow'] \n",
    "URBAN_TYPES = ['Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial']\n",
    "\n",
    "# Column groups\n",
    "COLUMNS_TO_DROP = ['geography_type', 'urban_type', 'geometry', 'date0', 'date1', 'date2', 'date3', 'date4']\n",
    "DATE_COLUMNS = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "\n",
    "# Feature types\n",
    "BINARY_FEATURES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow',\n",
    "                   'Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial'] \n",
    "CATEGORICAL_FEATURES = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3',\n",
    "                      'change_status_date4']\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = 'preprocessed_train.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "original_train_df = gpd.read_file(f'{BASE_PATH}/data/train.geojson', index_col=0)\n",
    "#test_df = gpd.read_file(f'{BASE_PATH}/data/test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data\n",
    "train_df = original_train_df.copy(deep=True)\n",
    "\n",
    "# Apply Mapping\n",
    "train_df['change_type'] = train_df['change_type'].map(CHANGE_TYPE_MAP)\n",
    "for i in range(5): train_df[f'change_status_date{i}'] = train_df[f'change_status_date{i}'].map(CHANGE_STATUS_MAP)\n",
    "\n",
    "# Fill missing data with 0\n",
    "train_df = train_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    train_df[geograph_type] = train_df['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "for urban_type in URBAN_TYPES:\n",
    "    train_df[urban_type] = train_df['urban_type'].apply(lambda x: 1 if urban_type in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['area'] = train_df['geometry'].area\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['length'] = train_df['geometry'].length\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['centroid_x'] = train_df['geometry'].centroid.x\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:5: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['centroid_y'] = train_df['geometry'].centroid.y\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:8: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(pd.to_datetime)\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:8: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(pd.to_datetime)\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:8: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(pd.to_datetime)\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:8: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(pd.to_datetime)\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_4252\\1349106329.py:8: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(pd.to_datetime)\n"
     ]
    }
   ],
   "source": [
    "# Create new polygon features\n",
    "train_df['area'] = train_df['geometry'].area\n",
    "train_df['length'] = train_df['geometry'].length\n",
    "train_df['centroid_x'] = train_df['geometry'].centroid.x\n",
    "train_df['centroid_y'] = train_df['geometry'].centroid.y\n",
    "\n",
    "# Create new date related features\n",
    "train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(pd.to_datetime)\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = train_df[f'img_{color}_{metric}_date{i}'] - train_df[f'img_{color}_{metric}_date{i-1}']\n",
    "            train_df[f'img_{color}_{metric}_delta{i}'] = delta\n",
    "        train_df[f'img_{color}_{metric}_delta_total'] = train_df[f'img_{color}_{metric}_date5'] - train_df[f'img_{color}_{metric}_date1']\n",
    "for i in range(1, 5):\n",
    "    train_df[f'date_delta{i}'] = (train_df[f'date{i}'] - train_df[f'date{i-1}']).dt.days.astype(int)\n",
    "train_df['date_delta_total'] = (train_df[f'date4'] - train_df[f'date1']).dt.days.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop uncessary columns\n",
    "train_df = train_df.drop(columns=COLUMNS_TO_DROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization of numeric features\n",
    "numeric_features = [col for col in train_df.columns if col not in BINARY_FEATURES + CATEGORICAL_FEATURES]\n",
    "numeric_features.remove('change_type')\n",
    "for col_name in numeric_features:\n",
    "    mean_value = train_df[col_name].mean()\n",
    "    std_value = train_df[col_name].std()\n",
    "    train_df[col_name] = (train_df[col_name] - mean_value) / std_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns[train_df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6664/6664 - 7s - loss: 1.0370 - accuracy: 0.5551 - val_loss: 0.8894 - val_accuracy: 0.6339 - 7s/epoch - 1ms/step\n",
      "Epoch 2/10\n",
      "6664/6664 - 6s - loss: 0.9226 - accuracy: 0.6133 - val_loss: 0.8516 - val_accuracy: 0.6494 - 6s/epoch - 901us/step\n",
      "Epoch 3/10\n",
      "6664/6664 - 6s - loss: 0.9009 - accuracy: 0.6276 - val_loss: 0.8404 - val_accuracy: 0.6543 - 6s/epoch - 893us/step\n",
      "Epoch 4/10\n",
      "6664/6664 - 6s - loss: 0.8914 - accuracy: 0.6301 - val_loss: 0.8435 - val_accuracy: 0.6560 - 6s/epoch - 862us/step\n",
      "Epoch 5/10\n",
      "6664/6664 - 7s - loss: 0.8861 - accuracy: 0.6335 - val_loss: 0.8274 - val_accuracy: 0.6580 - 7s/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "6664/6664 - 7s - loss: 0.8807 - accuracy: 0.6367 - val_loss: 0.8299 - val_accuracy: 0.6576 - 7s/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "6664/6664 - 6s - loss: 0.8793 - accuracy: 0.6371 - val_loss: 0.8257 - val_accuracy: 0.6565 - 6s/epoch - 952us/step\n",
      "Epoch 8/10\n",
      "6664/6664 - 6s - loss: 0.8755 - accuracy: 0.6388 - val_loss: 0.8223 - val_accuracy: 0.6606 - 6s/epoch - 875us/step\n",
      "Epoch 9/10\n",
      "6664/6664 - 6s - loss: 0.8731 - accuracy: 0.6395 - val_loss: 0.8164 - val_accuracy: 0.6603 - 6s/epoch - 883us/step\n",
      "Epoch 10/10\n",
      "6664/6664 - 6s - loss: 0.8696 - accuracy: 0.6415 - val_loss: 0.8115 - val_accuracy: 0.6616 - 6s/epoch - 889us/step\n",
      "1851/1851 [==============================] - 1s 673us/step - loss: 0.7994 - accuracy: 0.6685\n"
     ]
    }
   ],
   "source": [
    "# Define features and target variable\n",
    "X = np.array(train_df.drop('change_type', axis=1))\n",
    "y = np.array(train_df['change_type'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(516, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Original accuracy without feature selection\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=2)\n",
    "_, original_accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network with tree-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Select features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m importances_rf \u001b[38;5;241m=\u001b[39m model_rf\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m----> 7\u001b[0m selected_features_rf \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m[importances_rf \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.05\u001b[39m] \u001b[38;5;66;03m#ADJUST AS NEEDED\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train model and evaluate accuracy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_rf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchange_type\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[selected_features_rf])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Train random forest\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X, y)\n",
    "\n",
    "# Select features\n",
    "importances_rf = model_rf.feature_importances_\n",
    "selected_features_rf = train_df.columns[importances_rf > 0.05] #ADJUST AS NEEDED\n",
    "\n",
    "# Train model and evaluate accuracy\n",
    "X_rf = np.array(train_df.drop('change_type', axis=1)[selected_features_rf])\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_rf, y_train_rf, epochs=10, batch_size=32, validation_split=0.1, verbose=2)\n",
    "accuracy_rf = model.evaluate(X_rf, y_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04897911e-06, 1.22045434e-04, 3.20515363e-04, 4.03005988e-04,\n",
       "       4.69997253e-04, 7.30753138e-04, 1.07559690e-03, 1.23989401e-03,\n",
       "       1.26465342e-03, 1.30604251e-03, 1.38456977e-03, 1.45127151e-03,\n",
       "       1.52459177e-03, 1.52885532e-03, 2.23257786e-03, 5.37559599e-03,\n",
       "       8.98361593e-03, 9.09044365e-03, 9.10536103e-03, 9.18390725e-03,\n",
       "       9.18705059e-03, 9.18857899e-03, 9.20033518e-03, 9.24084851e-03,\n",
       "       9.24334547e-03, 9.26423038e-03, 9.26653853e-03, 9.35583821e-03,\n",
       "       9.37804865e-03, 9.48830878e-03, 9.49003235e-03, 9.49054055e-03,\n",
       "       9.49970214e-03, 9.50261245e-03, 9.50400438e-03, 9.52414735e-03,\n",
       "       9.53587806e-03, 9.53950973e-03, 9.67400214e-03, 9.67474122e-03,\n",
       "       9.70160209e-03, 9.72037811e-03, 9.72490179e-03, 9.72523285e-03,\n",
       "       9.79050794e-03, 9.83345891e-03, 9.86182764e-03, 9.91448667e-03,\n",
       "       9.92761394e-03, 9.94983621e-03, 9.96209902e-03, 9.97558526e-03,\n",
       "       1.00212124e-02, 1.00856644e-02, 1.01016046e-02, 1.01281580e-02,\n",
       "       1.01837357e-02, 1.01841648e-02, 1.01900371e-02, 1.01969143e-02,\n",
       "       1.03130340e-02, 1.03200408e-02, 1.03905693e-02, 1.04343701e-02,\n",
       "       1.05895470e-02, 1.07309257e-02, 1.07632144e-02, 1.10296509e-02,\n",
       "       1.11752619e-02, 1.13267800e-02, 1.15845409e-02, 1.18017817e-02,\n",
       "       1.23794067e-02, 1.24994136e-02, 1.26119913e-02, 1.32660639e-02,\n",
       "       1.43980266e-02, 1.44250032e-02, 1.47549936e-02, 1.52570297e-02,\n",
       "       1.55642543e-02, 1.82302273e-02, 1.87588717e-02, 1.96578495e-02,\n",
       "       2.17806478e-02, 2.18934909e-02, 2.25195868e-02, 3.23819617e-02,\n",
       "       3.97216637e-02, 4.76133604e-02, 5.76047821e-02])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6664/6664 - 16s - loss: 0.9241 - accuracy: 0.6062 - val_loss: 0.8262 - val_accuracy: 0.6509 - 16s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "6664/6664 - 14s - loss: 0.8485 - accuracy: 0.6503 - val_loss: 0.8009 - val_accuracy: 0.6691 - 14s/epoch - 2ms/step\n",
      "Epoch 3/20\n",
      "6664/6664 - 14s - loss: 0.8300 - accuracy: 0.6576 - val_loss: 0.7867 - val_accuracy: 0.6744 - 14s/epoch - 2ms/step\n",
      "Epoch 4/20\n",
      "6664/6664 - 15s - loss: 0.8203 - accuracy: 0.6623 - val_loss: 0.7806 - val_accuracy: 0.6815 - 15s/epoch - 2ms/step\n",
      "Epoch 5/20\n",
      "6664/6664 - 14s - loss: 0.8123 - accuracy: 0.6659 - val_loss: 0.7772 - val_accuracy: 0.6836 - 14s/epoch - 2ms/step\n",
      "Epoch 6/20\n",
      "6664/6664 - 14s - loss: 0.8103 - accuracy: 0.6658 - val_loss: 0.7703 - val_accuracy: 0.6816 - 14s/epoch - 2ms/step\n",
      "Epoch 7/20\n",
      "6664/6664 - 14s - loss: 0.8042 - accuracy: 0.6686 - val_loss: 0.7680 - val_accuracy: 0.6829 - 14s/epoch - 2ms/step\n",
      "Epoch 8/20\n",
      "6664/6664 - 15s - loss: 0.8012 - accuracy: 0.6698 - val_loss: 0.7685 - val_accuracy: 0.6830 - 15s/epoch - 2ms/step\n",
      "Epoch 9/20\n",
      "6664/6664 - 15s - loss: 0.7983 - accuracy: 0.6703 - val_loss: 0.7573 - val_accuracy: 0.6909 - 15s/epoch - 2ms/step\n",
      "Epoch 10/20\n",
      "6664/6664 - 15s - loss: 0.8012 - accuracy: 0.6710 - val_loss: 0.7610 - val_accuracy: 0.6904 - 15s/epoch - 2ms/step\n",
      "Epoch 11/20\n",
      "6664/6664 - 15s - loss: 0.7935 - accuracy: 0.6725 - val_loss: 0.7573 - val_accuracy: 0.6869 - 15s/epoch - 2ms/step\n",
      "Epoch 12/20\n",
      "6664/6664 - 15s - loss: 0.7905 - accuracy: 0.6742 - val_loss: 0.7589 - val_accuracy: 0.6902 - 15s/epoch - 2ms/step\n",
      "Epoch 13/20\n",
      "6664/6664 - 15s - loss: 0.7894 - accuracy: 0.6751 - val_loss: 0.7504 - val_accuracy: 0.6898 - 15s/epoch - 2ms/step\n",
      "Epoch 14/20\n",
      "6664/6664 - 15s - loss: 0.7862 - accuracy: 0.6759 - val_loss: 0.7457 - val_accuracy: 0.6934 - 15s/epoch - 2ms/step\n",
      "Epoch 15/20\n",
      "6664/6664 - 18s - loss: 0.7829 - accuracy: 0.6776 - val_loss: 0.7492 - val_accuracy: 0.6943 - 18s/epoch - 3ms/step\n",
      "Epoch 16/20\n",
      "6664/6664 - 16s - loss: 0.7862 - accuracy: 0.6786 - val_loss: 0.7635 - val_accuracy: 0.6856 - 16s/epoch - 2ms/step\n",
      "Epoch 17/20\n",
      "6664/6664 - 16s - loss: 0.7841 - accuracy: 0.6789 - val_loss: 0.7433 - val_accuracy: 0.6966 - 16s/epoch - 2ms/step\n",
      "Epoch 18/20\n",
      "6664/6664 - 17s - loss: 0.7819 - accuracy: 0.6791 - val_loss: 0.7412 - val_accuracy: 0.6970 - 17s/epoch - 2ms/step\n",
      "Epoch 19/20\n",
      "6664/6664 - 17s - loss: 0.7839 - accuracy: 0.6799 - val_loss: 0.7534 - val_accuracy: 0.6938 - 17s/epoch - 3ms/step\n",
      "Epoch 20/20\n",
      "6664/6664 - 15s - loss: 0.7798 - accuracy: 0.6800 - val_loss: 0.7465 - val_accuracy: 0.6936 - 15s/epoch - 2ms/step\n",
      "1851/1851 [==============================] - 2s 1ms/step - loss: 0.7395 - accuracy: 0.6988\n"
     ]
    }
   ],
   "source": [
    "# Calculate PCA\n",
    "pca = PCA(n_components=80) # ADJUST\n",
    "\n",
    "# Define features and target variable\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and compile model\n",
    "pca_model = keras.Sequential([\n",
    "    keras.layers.Dense(512, activation='relu', input_shape=(X_train_pca.shape[1],)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(128, activation='relu'),  # Additional layer\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "pca_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model and evaluate accuracy\n",
    "pca_model.fit(X_train_pca, y_train_pca, epochs=20, batch_size=32, validation_split=0.1, verbose=2)\n",
    "_, original_accuracy = pca_model.evaluate(X_test_pca, y_test_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
