{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2EL1730 Machine Learning Project - Jan. 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sb\n",
    "from datetime import date\n",
    "import geopandas as gpd\n",
    "import seaborn as sn\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import recall_score, precision_score,accuracy_score, average_precision_score, f1_score, log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Mapping\n",
    "CHANGE_TYPE_MAP = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5}\n",
    "CHANGE_STATUS_MAP = {'Greenland': 0, 'Land Cleared': 1, 'Excavation': 1, 'Materials Dumped': 3, 'Prior Construction': 3, 'Materials Introduced': 4, 'Construction Started': 5, 'Construction Midway': 6, 'Construction Done': 8, 'Operational': 10, None: None}\n",
    "\n",
    "# Data\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "METRICS = ['std', 'mean']\n",
    "GEOGRAPHY_TYPES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow'] \n",
    "URBAN_TYPES = ['Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial']\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,'Mega Projects': 5}\n",
    "\n",
    "# Columns groups\n",
    "COLUMNS_TO_DROP = ['geography_type', 'urban_type']\n",
    "DATE_COLUMNS = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "CHANGE_STATUS_COLUMNS = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3', 'change_status_date4']\n",
    "CHANGE_STATUS_VALUE_COLUMNS = ['change_status_value_date0', 'change_status_value_date1', 'change_status_value_date2', 'change_status_value_date3', 'change_status_value_date4']\n",
    "\n",
    "# Feature types\n",
    "BINARY_FEATURES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow',\n",
    "                   'Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial']\n",
    "CATEGORICAL_FEATURES = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3',\n",
    "                      'change_status_date4']\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = 'preprocessed_train.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = gpd.read_file(f'{BASE_PATH}/data/train.geojson', index_col=0)\n",
    "test_df = gpd.read_file(f'{BASE_PATH}/data/test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n"
     ]
    }
   ],
   "source": [
    "train_df_pre_process = train_df.copy()\n",
    "\n",
    "# Fill missing data with 0\n",
    "train_df_pre_process = train_df_pre_process.fillna(0)\n",
    "\n",
    "# One-hot encoding for geography_type\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    train_df_pre_process[geograph_type] = train_df_pre_process['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "\n",
    "for urban_type in URBAN_TYPES:\n",
    "    train_df_pre_process[urban_type] = train_df_pre_process['urban_type'].apply(lambda x: 1 if urban_type in x else 0)\n",
    "\n",
    "## Create new polygon features\n",
    "train_df_pre_process['area'] = train_df_pre_process['geometry'].area\n",
    "train_df_pre_process['length'] = train_df_pre_process['geometry'].length\n",
    "#train_df_pre_process['area_to_length_ratio'] = train_df_pre_process['area'] / train_df_pre_process['length'] # the more, the closer to square\n",
    "train_df_pre_process['centroid_x'] = train_df_pre_process['geometry'].centroid.x\n",
    "train_df_pre_process['centroid_y'] = train_df_pre_process['geometry'].centroid.y\n",
    "\n",
    "'''\n",
    "### Convert date from string to date\n",
    "train_df_pre_process[DATE_COLUMNS] = train_df_pre_process[DATE_COLUMNS].apply(lambda x: pd.to_datetime(x, format='%d-%m-%Y', errors='coerce'))\n",
    "\n",
    "## Create deltas color[std, mean] \n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = train_df_pre_process[f'img_{color}_{metric}_date{i}'] - train_df_pre_process[f'img_{color}_{metric}_date{i-1}']\n",
    "            train_df_pre_process[f'img_{color}_{metric}_delta{i}'] = delta\n",
    "        train_df_pre_process[f'img_{color}_{metric}_delta_total'] = train_df_pre_process[f'img_{color}_{metric}_date5'] - train_df[f'img_{color}_{metric}_date1']\n",
    "\n",
    "## Create deltas time \n",
    "for i in range(1, 5):\n",
    "    train_df_pre_process[f'date_delta{i}'] = train_df_pre_process[f'date{i}'] - train_df_pre_process[f'date{i-1}']\n",
    "train_df_pre_process['date_delta_total'] = train_df_pre_process[f'date4'] - train_df_pre_process[f'date0']\n",
    "\n",
    "## Standardizing colors mean by the proportion\n",
    "for i in range(1, 6):\n",
    "    color_sum = train_df_pre_process[f'img_blue_mean_date{i}'] + train_df_pre_process[f'img_green_mean_date{i}'] + train_df_pre_process[f'img_red_mean_date{i}']\n",
    "    for color in COLORS:\n",
    "        train_df_pre_process[f'img_{color}_mean_prop_date{i}'] = train_df_pre_process[f'img_{color}_mean_date{i}']/color_sum\n",
    "\n",
    "## Create img_{color}_mean_prop_rate\n",
    "num_samples = train_df_pre_process.shape[0]\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "for color in COLORS:\n",
    "    coef = np.zeros((num_samples))\n",
    "    COLOR_MEAN_COLUMNS = [f'img_{color}_mean_prop_date{i}' for i in range (1,6)]\n",
    "    \n",
    "    Y = np.array(train_df_pre_process[COLOR_MEAN_COLUMNS].astype(float))\n",
    "    nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "    X = np.array(train_df_pre_process[DATE_COLUMNS].astype(int))[:,:,np.newaxis]/time_ctt\n",
    "    X = np.dstack((ones,X))\n",
    "    X[nan_mask,:] = 0\n",
    "    Y[nan_mask] = 0\n",
    "\n",
    "    eye = np.eye(2)*0.0001\n",
    "    for i in range(num_samples):\n",
    "        x = X[i].reshape((5,2))\n",
    "        y = Y[i].reshape((5))\n",
    "        coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "        \n",
    "    train_df_pre_process[f'img_{color}_mean_prop_rate'] = coef\n",
    "'''\n",
    "\n",
    "# Create new date related features\n",
    "train_df_pre_process[DATE_COLUMNS] = train_df_pre_process[DATE_COLUMNS].apply(pd.to_datetime)\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = train_df_pre_process[f'img_{color}_{metric}_date{i}'] - train_df_pre_process[f'img_{color}_{metric}_date{i-1}']\n",
    "            train_df_pre_process[f'img_{color}_{metric}_delta{i}'] = delta\n",
    "        train_df_pre_process[f'img_{color}_{metric}_delta_total'] = train_df_pre_process[f'img_{color}_{metric}_date5'] - train_df_pre_process[f'img_{color}_{metric}_date1']\n",
    "for i in range(1, 5):\n",
    "    train_df_pre_process[f'date_delta{i}'] = train_df_pre_process[f'date{i}'] - train_df_pre_process[f'date{i-1}']\n",
    "train_df_pre_process['date_delta_total'] = train_df_pre_process[f'date4'] - train_df_pre_process[f'date1']\n",
    "\n",
    "# Map change_type\n",
    "train_df_pre_process['change_type'] = train_df_pre_process['change_type'].map(change_type_map)\n",
    "for i in range(5): train_df_pre_process[f'change_status_date{i}'] = train_df_pre_process[f'change_status_date{i}'].map(CHANGE_STATUS_MAP)\n",
    "\n",
    "train_df_pre_process['date_delta_total'] = train_df_pre_process['date_delta_total']/np.timedelta64(1, 'D')\n",
    "train_df_pre_process['date_delta1'] = train_df_pre_process['date_delta1']/np.timedelta64(1, 'D')\n",
    "train_df_pre_process['date_delta2'] = train_df_pre_process['date_delta2']/np.timedelta64(1, 'D')\n",
    "train_df_pre_process['date_delta3'] = train_df_pre_process['date_delta3']/np.timedelta64(1, 'D')\n",
    "train_df_pre_process['date_delta4'] = train_df_pre_process['date_delta4']/np.timedelta64(1, 'D')\n",
    "\n",
    "# Drop uncessary columns\n",
    "train_df_pre_process = train_df_pre_process.drop(columns=COLUMNS_TO_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 296146 entries, 0 to 296145\n",
      "Data columns (total 92 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   change_type                 296146 non-null  int64  \n",
      " 1   img_red_mean_date1          296146 non-null  float64\n",
      " 2   img_green_mean_date1        296146 non-null  float64\n",
      " 3   img_blue_mean_date1         296146 non-null  float64\n",
      " 4   img_red_std_date1           296146 non-null  float64\n",
      " 5   img_green_std_date1         296146 non-null  float64\n",
      " 6   img_blue_std_date1          296146 non-null  float64\n",
      " 7   img_red_mean_date2          296146 non-null  float64\n",
      " 8   img_green_mean_date2        296146 non-null  float64\n",
      " 9   img_blue_mean_date2         296146 non-null  float64\n",
      " 10  img_red_std_date2           296146 non-null  float64\n",
      " 11  img_green_std_date2         296146 non-null  float64\n",
      " 12  img_blue_std_date2          296146 non-null  float64\n",
      " 13  img_red_mean_date3          296146 non-null  float64\n",
      " 14  img_green_mean_date3        296146 non-null  float64\n",
      " 15  img_blue_mean_date3         296146 non-null  float64\n",
      " 16  img_red_std_date3           296146 non-null  float64\n",
      " 17  img_green_std_date3         296146 non-null  float64\n",
      " 18  img_blue_std_date3          296146 non-null  float64\n",
      " 19  img_red_mean_date4          296146 non-null  float64\n",
      " 20  img_green_mean_date4        296146 non-null  float64\n",
      " 21  img_blue_mean_date4         296146 non-null  float64\n",
      " 22  img_red_std_date4           296146 non-null  float64\n",
      " 23  img_green_std_date4         296146 non-null  float64\n",
      " 24  img_blue_std_date4          296146 non-null  float64\n",
      " 25  img_red_mean_date5          296146 non-null  float64\n",
      " 26  img_green_mean_date5        296146 non-null  float64\n",
      " 27  img_blue_mean_date5         296146 non-null  float64\n",
      " 28  img_red_std_date5           296146 non-null  float64\n",
      " 29  img_green_std_date5         296146 non-null  float64\n",
      " 30  img_blue_std_date5          296146 non-null  float64\n",
      " 31  change_status_date0         296146 non-null  float64\n",
      " 32  change_status_date1         296146 non-null  float64\n",
      " 33  change_status_date2         296146 non-null  float64\n",
      " 34  change_status_date3         296146 non-null  float64\n",
      " 35  change_status_date4         296146 non-null  float64\n",
      " 36  index                       296146 non-null  int64  \n",
      " 37  centroid_x                  296146 non-null  float64\n",
      " 38  centroid_y                  296146 non-null  float64\n",
      " 39  Dense Forest                296146 non-null  int64  \n",
      " 40  Grass Land                  296146 non-null  int64  \n",
      " 41  Sparse Forest               296146 non-null  int64  \n",
      " 42  Farms                       296146 non-null  int64  \n",
      " 43  River                       296146 non-null  int64  \n",
      " 44  Coastal                     296146 non-null  int64  \n",
      " 45  Lakes                       296146 non-null  int64  \n",
      " 46  Barren Land                 296146 non-null  int64  \n",
      " 47  Desert                      296146 non-null  int64  \n",
      " 48  Hills                       296146 non-null  int64  \n",
      " 49  Snow                        296146 non-null  int64  \n",
      " 50  Sparse Urban                296146 non-null  int64  \n",
      " 51  Rural                       296146 non-null  int64  \n",
      " 52  Dense Urban                 296146 non-null  int64  \n",
      " 53  Urban Slum                  296146 non-null  int64  \n",
      " 54  Industrial                  296146 non-null  int64  \n",
      " 55  area                        296146 non-null  float64\n",
      " 56  length                      296146 non-null  float64\n",
      " 57  img_red_std_delta2          296146 non-null  float64\n",
      " 58  img_red_std_delta3          296146 non-null  float64\n",
      " 59  img_red_std_delta4          296146 non-null  float64\n",
      " 60  img_red_std_delta5          296146 non-null  float64\n",
      " 61  img_red_std_delta_total     296146 non-null  float64\n",
      " 62  img_green_std_delta2        296146 non-null  float64\n",
      " 63  img_green_std_delta3        296146 non-null  float64\n",
      " 64  img_green_std_delta4        296146 non-null  float64\n",
      " 65  img_green_std_delta5        296146 non-null  float64\n",
      " 66  img_green_std_delta_total   296146 non-null  float64\n",
      " 67  img_blue_std_delta2         296146 non-null  float64\n",
      " 68  img_blue_std_delta3         296146 non-null  float64\n",
      " 69  img_blue_std_delta4         296146 non-null  float64\n",
      " 70  img_blue_std_delta5         296146 non-null  float64\n",
      " 71  img_blue_std_delta_total    296146 non-null  float64\n",
      " 72  img_red_mean_delta2         296146 non-null  float64\n",
      " 73  img_red_mean_delta3         296146 non-null  float64\n",
      " 74  img_red_mean_delta4         296146 non-null  float64\n",
      " 75  img_red_mean_delta5         296146 non-null  float64\n",
      " 76  img_red_mean_delta_total    296146 non-null  float64\n",
      " 77  img_green_mean_delta2       296146 non-null  float64\n",
      " 78  img_green_mean_delta3       296146 non-null  float64\n",
      " 79  img_green_mean_delta4       296146 non-null  float64\n",
      " 80  img_green_mean_delta5       296146 non-null  float64\n",
      " 81  img_green_mean_delta_total  296146 non-null  float64\n",
      " 82  img_blue_mean_delta2        296146 non-null  float64\n",
      " 83  img_blue_mean_delta3        296146 non-null  float64\n",
      " 84  img_blue_mean_delta4        296146 non-null  float64\n",
      " 85  img_blue_mean_delta5        296146 non-null  float64\n",
      " 86  img_blue_mean_delta_total   296146 non-null  float64\n",
      " 87  date_delta1                 296146 non-null  float64\n",
      " 88  date_delta2                 296146 non-null  float64\n",
      " 89  date_delta3                 296146 non-null  float64\n",
      " 90  date_delta4                 296146 non-null  float64\n",
      " 91  date_delta_total            296146 non-null  float64\n",
      "dtypes: float64(74), int64(18)\n",
      "memory usage: 207.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Fill missing data with 0\n",
    "train_df_pre_process = train_df_pre_process.fillna(0)\n",
    "train_df_pre_process.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of numeric features and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y\n",
    "X = np.array(train_df_pre_process.drop('change_type', axis=1))\n",
    "y = np.array(train_df_pre_process['change_type'])\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Indices of numeric and categorical columns\n",
    "numeric_features = [i for i in range(X.shape[1]) if train_df_pre_process.drop(columns = ['change_type']).columns[i] not in BINARY_FEATURES + CATEGORICAL_FEATURES]\n",
    "categorical_features = [i for i in range(X.shape[1]) if train_df_pre_process.columns[i] in CATEGORICAL_FEATURES]\n",
    "binary_features = [i for i in range(X.shape[1]) if train_df_pre_process.columns[i] in BINARY_FEATURES]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', 'passthrough', categorical_features),\n",
    "        ('binary', 'passthrough', binary_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data\n",
    "X_val_scaled = preprocessor.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n"
     ]
    }
   ],
   "source": [
    "test_df_pre_process = test_df.copy()\n",
    "\n",
    "# Fill missing data with 0\n",
    "test_df_pre_process = test_df_pre_process.fillna(0)\n",
    "\n",
    "# One-hot encoding for geography_type\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    test_df_pre_process[geograph_type] = test_df_pre_process['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "\n",
    "for urban_type in URBAN_TYPES:\n",
    "    test_df_pre_process[urban_type] = test_df_pre_process['urban_type'].apply(lambda x: 1 if urban_type in x else 0)\n",
    "\n",
    "# Create new polygon features\n",
    "test_df_pre_process['area'] = test_df_pre_process['geometry'].area\n",
    "test_df_pre_process['length'] = test_df_pre_process['geometry'].length\n",
    "test_df_pre_process['centroid_x'] = test_df_pre_process['geometry'].centroid.x\n",
    "test_df_pre_process['centroid_y'] = test_df_pre_process['geometry'].centroid.y\n",
    "\n",
    "# Create new date related features\n",
    "test_df_pre_process[DATE_COLUMNS] = test_df_pre_process[DATE_COLUMNS].apply(pd.to_datetime)\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = test_df_pre_process[f'img_{color}_{metric}_date{i}'] - test_df_pre_process[f'img_{color}_{metric}_date{i-1}']\n",
    "            test_df_pre_process[f'img_{color}_{metric}_delta{i}'] = delta\n",
    "        test_df_pre_process[f'img_{color}_{metric}_delta_total'] = test_df_pre_process[f'img_{color}_{metric}_date5'] - test_df_pre_process[f'img_{color}_{metric}_date1']\n",
    "for i in range(1, 5):\n",
    "    test_df_pre_process[f'date_delta{i}'] = test_df_pre_process[f'date{i}'] - test_df_pre_process[f'date{i-1}']\n",
    "test_df_pre_process['date_delta_total'] = test_df_pre_process[f'date4'] - test_df_pre_process[f'date1']\n",
    "\n",
    "for i in range(5): test_df_pre_process[f'change_status_date{i}'] = test_df_pre_process[f'change_status_date{i}'].map(CHANGE_STATUS_MAP)\n",
    "\n",
    "test_df_pre_process['date_delta_total'] = test_df_pre_process['date_delta_total']/np.timedelta64(1, 'D')\n",
    "test_df_pre_process['date_delta1'] = test_df_pre_process['date_delta1']/np.timedelta64(1, 'D')\n",
    "test_df_pre_process['date_delta2'] = test_df_pre_process['date_delta2']/np.timedelta64(1, 'D')\n",
    "test_df_pre_process['date_delta3'] = test_df_pre_process['date_delta3']/np.timedelta64(1, 'D')\n",
    "test_df_pre_process['date_delta4'] = test_df_pre_process['date_delta4']/np.timedelta64(1, 'D')\n",
    "\n",
    "# Drop uncessary columns\n",
    "test_df_pre_process = test_df_pre_process.drop(columns=COLUMNS_TO_DROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pre_process = test_df_pre_process.fillna(0)\n",
    "\n",
    "numeric_features = [col for col in test_df_pre_process.columns if col not in BINARY_FEATURES + CATEGORICAL_FEATURES]\n",
    "for col_name in numeric_features:\n",
    "    mean_value = test_df_pre_process[col_name].mean()\n",
    "    std_value = test_df_pre_process[col_name].std()\n",
    "    test_df_pre_process[col_name] = (test_df_pre_process[col_name] - mean_value) / std_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 148435, 3: 100422, 0: 31509, 1: 14305, 4: 1324, 5: 151})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on validation data: 0.5027182171197029\n",
      "Model F1-score on validation data: 0.33635794514878264\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "X_test = np.array(test_df_pre_process)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the accuracy of the model on the validation data\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Model accuracy on validation data: {accuracy}')\n",
    "\n",
    "# Calculate and print the F1-score\n",
    "f1 = f1_score(y_val, y_pred , average='weighted')\n",
    "print(f'Model F1-score on validation data: {f1}')\n",
    "\n",
    "'''\n",
    "# 5 - fold cross validation\n",
    "n_scores = cross_val_score(model, X, y, scoring= 'f1', cv= RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1), n_jobs=-1)\n",
    "# report the model performance\n",
    "print('Mean F1-score: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "'''\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "## Save results to submission file\n",
    "pred_df = pd.DataFrame(y_pred_test, columns=['change_type'])\n",
    "#pred_df_LR.to_csv(\"LR_submission.csv\", index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with feature selection - takes a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Embedded methods to feature selection\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sel \u001b[38;5;241m=\u001b[39m SelectFromModel(RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m), threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m sel\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[0;32m      8\u001b[0m X_train_selected \u001b[38;5;241m=\u001b[39m sel\u001b[38;5;241m.\u001b[39mtransform(X_train_scaled)\n\u001b[0;32m      9\u001b[0m X_val_selected \u001b[38;5;241m=\u001b[39m sel\u001b[38;5;241m.\u001b[39mtransform(X_val_scaled)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:358\u001b[0m, in \u001b[0;36mSelectFromModel.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_ \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_\u001b[38;5;241m.\u001b[39mfeature_names_in_\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "#test data\n",
    "X_test = np.array(test_df_pre_process)\n",
    "\n",
    "#Embedded methods to feature selection\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators= 100, random_state=10), threshold='median')\n",
    "sel.fit(X_train_scaled, y_train)\n",
    "\n",
    "X_train_selected = sel.transform(X_train_scaled)\n",
    "X_val_selected = sel.transform(X_val_scaled)\n",
    "X_test_selected = sel.transform(X_test)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model_LR = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "#model_LR = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, class_weight = 'balanced')\n",
    "\n",
    "# Train the model\n",
    "model_LR.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = model_LR.predict(X_val_selected)\n",
    "\n",
    "# Evaluate the accuracy of the model on the validation data\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Model accuracy on validation data: {accuracy}')\n",
    "\n",
    "# Calculate and print the F1-score\n",
    "f1 = f1_score(y_val, y_pred , average='weighted')\n",
    "print(f'Model F1-score on validation data: {f1}')\n",
    "\n",
    "# Make predictions on the test data\n",
    "pred_y = model_LR.predict(X_test_selected)\n",
    "\n",
    "## Save results to submission file\n",
    "pred_df = pd.DataFrame(pred_y, columns=['change_type'])\n",
    "#pred_df_LR.to_csv(\"LR_submission.csv\", index=True, index_label='Id')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
