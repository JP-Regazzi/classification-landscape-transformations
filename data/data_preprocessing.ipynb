{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Mapping\n",
    "CHANGE_TYPE_MAP = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5}\n",
    "CHANGE_STATUS_MAP = {'Greenland': 0.0, 'Land Cleared': 1.0, 'Excavation': 1.0, 'Materials Dumped': 3.0, 'Prior Construction': 3.0, 'Materials Introduced': 4.0, 'Construction Started': 5.0, 'Construction Midway': 6.0, 'Construction Done': 8.0, 'Operational': 10.0, None: None}\n",
    "\n",
    "# Data\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "METRICS = ['std', 'mean']\n",
    "GEOGRAPHY_TYPES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow'] \n",
    "URBAN_TYPES = ['Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial']\n",
    "\n",
    "# Columns groups\n",
    "COLUMNS_TO_DROP = []\n",
    "DATE_COLUMNS = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "CHANGE_STATUS_COLUMNS = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3', 'change_status_date4']\n",
    "CHANGE_STATUS_VALUE_COLUMNS = ['change_status_value_date0', 'change_status_value_date1', 'change_status_value_date2', 'change_status_value_date3', 'change_status_value_date4']\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = 'preprocessed_train.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "train_df_read = gpd.read_file(f'{BASE_PATH}/data/train.geojson', index_col=0)\n",
    "#test_df = gpd.read_file(f'{BASE_PATH}/data/test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_read.copy(deep=True)\n",
    "# Safety\n",
    "MAPPED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean samples with NaN in column  \"change_type\"\n",
    "#df_cleaned = df.dropna(subset=['change_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map change_type\n",
    "if ~MAPPED:\n",
    "    train_df['change_type'] = train_df['change_type'].map(CHANGE_TYPE_MAP)\n",
    "    MAPPED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot encoding\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    train_df[geograph_type] = train_df['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "for urban_type in URBAN_TYPES:\n",
    "    train_df[urban_type] = train_df['urban_type'].apply(lambda x: 1 if urban_type in x else 0)\n",
    "## DROP\n",
    "train_df = train_df.drop(columns=['geography_type', 'urban_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create change_status_values\n",
    "for i in range(len(CHANGE_STATUS_COLUMNS)):\n",
    "    train_df[CHANGE_STATUS_VALUE_COLUMNS[i]] = train_df[CHANGE_STATUS_COLUMNS[i]].map(CHANGE_STATUS_MAP)\n",
    "## DROP\n",
    "train_df = train_df.drop(columns=CHANGE_STATUS_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6973/2799808815.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['area'] = train_df['geometry'].area\n",
      "/tmp/ipykernel_6973/2799808815.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['length'] = train_df['geometry'].length\n",
      "/tmp/ipykernel_6973/2799808815.py:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['centroid_x'] = train_df['geometry'].centroid.x\n",
      "/tmp/ipykernel_6973/2799808815.py:5: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['centroid_y'] = train_df['geometry'].centroid.y\n"
     ]
    }
   ],
   "source": [
    "## Geometric features\n",
    "from shapely.geometry import Polygon, LinearRing\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "copied_train_df_save['geometry'].make_valid()\n",
    "\n",
    "## Create new polygon features\n",
    "copied_train_df_save['area'] = copied_train_df_save['geometry'].area\n",
    "copied_train_df_save['length'] = copied_train_df_save['geometry'].length\n",
    "copied_train_df_save['centroid_x'] = copied_train_df_save['geometry'].centroid.x\n",
    "copied_train_df_save['centroid_y'] = copied_train_df_save['geometry'].centroid.y\n",
    "bounds = copied_train_df_save['geometry'].bounds\n",
    "copied_train_df_save['angle'] = np.arctan((bounds['maxy']-bounds['miny'])/(bounds['maxx']-bounds['minx']))\n",
    "copied_train_df_save['compactness'] = 4 * np.pi * (copied_train_df_save['area'] / copied_train_df_save['length']**2) # the bigger, the closer to square\n",
    "\n",
    "def is_paralelogram(polygon):\n",
    "    it_is = True\n",
    "    LIMIT = 1e-2\n",
    "    # Get the outer boundary coordinates\n",
    "    coords = polygon.exterior.coords[:-1]  # Exclude the closing coordinate\n",
    "    \n",
    "    # Check if the polygon has four sides\n",
    "    num_vertices = len(coords)\n",
    "    if  num_vertices != 4:  # a paralelogram should have 4 vertices\n",
    "        it_is = False\n",
    "        return (it_is, num_vertices, 10*LIMIT, num_vertices*4*LIMIT)\n",
    "\n",
    "    # Calculate the lengths of the sides\n",
    "    side_lengths = [euclidean(coords[i],coords[(i+1)%4]) for i in range(4)]  # Calculate the length between each pair of adjacent vertices\n",
    "    \n",
    "    # Check if opposite sides have equal length (index 0 and 2, index 1 and 3)\n",
    "    length_dif = abs(side_lengths[0] - side_lengths[2]) + abs(side_lengths[1] - side_lengths[3])/polygon.length\n",
    "    if length_dif > LIMIT:\n",
    "        it_is = False\n",
    "    \n",
    "    return (it_is, num_vertices, length_dif, length_dif)\n",
    "\n",
    "tmp = copied_train_df_save['geometry'].apply(is_paralelogram)\n",
    "# Extract the tuple into three different columns\n",
    "copied_train_df_save['paralelogram'], copied_train_df_save['num_vertices'], copied_train_df_save['length_dif1'], copied_train_df_save['length_dif2'] = zip(*tmp)\n",
    "\n",
    "copied_train_df_save['rect_area'] = copied_train_df_save['geometry'].minimum_rotated_rectangle().area\n",
    "convex_hull = copied_train_df_save['geometry'].convex_hull\n",
    "copied_train_df_save['dif_convex_prop_area'] = (convex_hull.area - copied_train_df_save['area'])/ convex_hull.area\n",
    "copied_train_df_save['convex'] =  copied_train_df_save['geometry'].geom_equals(convex_hull)\n",
    "\n",
    "## DROP\n",
    "copied_train_df_save = copied_train_df_save.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert date from string to seconds\n",
    "train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(lambda x: pd.to_datetime(x, format='%d-%m-%Y', errors='coerce')).apply(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NaN values have been imputed successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Group the DataFrame by \"change_type\"\n",
    "grouped = train_df.groupby(\"change_type\")\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5,missing_values=np.nan)  # You can adjust the number of neighbors as needed\n",
    "\n",
    "# Initialize an empty list to store the imputed DataFrames\n",
    "imputed_dfs = []\n",
    "\n",
    "# Iterate over each group\n",
    "for change_type, group in grouped:\n",
    "    # Drop the \"change_type\" column before imputation\n",
    "    group_features = group.drop(columns=[\"change_type\"])\n",
    "    \n",
    "    # Impute NaN values within the group\n",
    "    imputed_values = imputer.fit_transform(group_features)\n",
    "    \n",
    "    # Create a DataFrame with imputed values\n",
    "    imputed_df = pd.DataFrame(imputed_values, columns=group_features.columns, index=group_features.index)\n",
    "    \n",
    "    # Concatenate \"change_type\" column back to the imputed DataFrame\n",
    "    imputed_df[\"change_type\"] = change_type\n",
    "    \n",
    "    # Append the imputed DataFrame to the list\n",
    "    imputed_dfs.append(imputed_df)\n",
    "\n",
    "# Concatenate all imputed DataFrames into a single DataFrame\n",
    "imputed_train_df = pd.concat(imputed_dfs)\n",
    "train_df = imputed_train_df.copy()\n",
    "\n",
    "# Verify if there are still NaN values after imputation\n",
    "if train_df.isnull().values.any():\n",
    "    print(\"There are still NaN values remaining after imputation.\")\n",
    "    print(train_df[train_df.isnull().any(axis=1)])\n",
    "else:\n",
    "    print(\"All NaN values have been imputed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIX DATES OUT OF ORDER\n",
    "def sort_dates(row):\n",
    "\n",
    "    # Sort columns by date\n",
    "    columns_order = np.argsort(row[DATE_COLUMNS].values)\n",
    "    new_row = row.copy(deep=True)\n",
    "\n",
    "    # Update date and change_status order\n",
    "    for i in range(5):\n",
    "        new_row[f'date{i}'] = row[f'date{columns_order[i]}']\n",
    "        new_row[f'change_status_value_date{i}'] = row[f'change_status_value_date{columns_order[i]}']\n",
    "\n",
    "    # Update color metrics order\n",
    "    for metric in METRICS:\n",
    "        for color in COLORS:\n",
    "            for i in range(1, 6):\n",
    "                new_row[f'img_{color}_{metric}_date{i}'] = row[f'img_{color}_{metric}_date{columns_order[i-1]+1}']\n",
    "\n",
    "    return new_row\n",
    "\n",
    "train_df = train_df.apply(sort_dates, axis=1)\n",
    "train_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['date0'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create deltas color[std, mean] \n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = train_df[f'img_{color}_{metric}_date{i}'] - train_df[f'img_{color}_{metric}_date{i-1}']\n",
    "            train_df[f'img_{color}_{metric}_delta{i}'] = delta.apply(np.float64)\n",
    "        train_df[f'img_{color}_{metric}_delta_total'] = (train_df[f'img_{color}_{metric}_date5'] - train_df[f'img_{color}_{metric}_date1']).apply(np.float64)\n",
    "\n",
    "## Create deltas time \n",
    "for i in range(1, 5):\n",
    "    train_df[f'date_delta{i}'] = (train_df[f'date{i}'] - train_df[f'date{i-1}']).apply(np.float64)\n",
    "train_df['date_delta_total'] = (train_df[f'date4'] - train_df[f'date0']).apply(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardizing colors mean by the proportion\n",
    "for i in range(1, 6):\n",
    "    color_sum = train_df[f'img_blue_mean_date{i}'] + train_df[f'img_green_mean_date{i}'] + train_df[f'img_red_mean_date{i}']\n",
    "    for color in COLORS:\n",
    "        train_df[f'img_{color}_mean_prop_date{i}'] = train_df[f'img_{color}_mean_date{i}']/color_sum\n",
    "\n",
    "## Create img_{color}_mean_prop_rate\n",
    "num_samples = train_df.shape[0]\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "for color in COLORS:\n",
    "    coef = np.zeros((num_samples))\n",
    "    COLOR_MEAN_COLUMNS = [f'img_{color}_mean_prop_date{i}' for i in range (1,6)]\n",
    "    \n",
    "    Y = np.array(train_df[COLOR_MEAN_COLUMNS].astype(float))\n",
    "    X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "    X = np.dstack((ones,X))\n",
    "    nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "    X[nan_mask,:] = 0\n",
    "    Y[nan_mask] = 0\n",
    "\n",
    "    eye = np.eye(2)*0.0001\n",
    "    for i in range(num_samples):\n",
    "        x = X[i].reshape((5,2))\n",
    "        y = Y[i].reshape((5))\n",
    "        coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "        \n",
    "    train_df[f'img_{color}_mean_prop_rate'] = coef\n",
    "    \n",
    "## Create img_{color}_{metric}_rate\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        coef = np.zeros((num_samples))\n",
    "        COLOR_METRIC_COLUMNS = [f'img_{color}_{metric}_date{i}' for i in range (1,6)]\n",
    "        \n",
    "        Y = np.array(train_df[COLOR_METRIC_COLUMNS].astype(float))\n",
    "        X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "        X = np.dstack((ones,X))\n",
    "        nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "        X[nan_mask,:] = 0\n",
    "        Y[nan_mask] = 0\n",
    "\n",
    "        eye = np.eye(2)*0.0001\n",
    "        for i in range(num_samples):\n",
    "            x = X[i].reshape((5,2))\n",
    "            y = Y[i].reshape((5))\n",
    "            coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "            \n",
    "        train_df[f'img_{color}_{metric}_rate'] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create civilization_rate\n",
    "num_samples = train_df.shape[0]\n",
    "coef = np.zeros((num_samples))\n",
    "time_ctt = 1e9*60*90*24\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "Y = np.array(train_df[CHANGE_STATUS_VALUE_COLUMNS].astype(float))\n",
    "Y_nan_mask = np.isnan(Y)\n",
    "X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]/time_ctt\n",
    "X = np.dstack((ones,X))\n",
    "X[Y_nan_mask,:] = 0\n",
    "Y[Y_nan_mask] = 0\n",
    "\n",
    "eye = np.eye(2)*0.0001\n",
    "for i in range(num_samples):\n",
    "    x = X[i].reshape((5,2))\n",
    "    y = Y[i].reshape((5))\n",
    "    coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "    #print(y, train_df[\"change_type\"].iloc[i])\n",
    "train_df[\"civilizating_rate\"] = coef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalization and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop uncessary columns\n",
    "#train_df = train_df.drop(columns=COLUMNS_TO_DROP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['date_delta1'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANOVA and information gain for each feature\n",
    "from scipy.stats import f_oneway\n",
    "#from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Assuming 'change_type' is the column name for classification effects\n",
    "change_type_column = 'change_type'\n",
    "#CONTINUOUS_FEATURES = ['area_to_length_ratio' ,'area' ,'angle' ,'compactness', 'length']\n",
    "\n",
    "# Group the data by 'change_type'\n",
    "grouped_data = train_df.groupby(change_type_column)\n",
    "\n",
    "# List to store F-values\n",
    "f_values = []\n",
    "#gain_of_info = []\n",
    "\n",
    "# Loop through each feature (column) and calculate F-value\n",
    "for feature_column in train_df.columns:\n",
    "    if feature_column != change_type_column and feature_column != 'geometry':  \n",
    "        # ANOVA\n",
    "        nan_mask = np.isnan(train_df[feature_column])\n",
    "        feature_values = [group[feature_column].dropna() for _, group in grouped_data]\n",
    "        f_statistic, p_value = f_oneway(*feature_values)\n",
    "        # Gain of Info\n",
    "        \n",
    "        \n",
    "        #discrete_bool = feature_column not in CONTINUOUS_FEATURES and 'std' not in feature_column and 'rate' not in feature_column and 'prop' not in feature_column\n",
    "        #gofinfo = mutual_info_classif(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask].astype(str),discrete_features=discrete_bool)\n",
    "        \n",
    "         \n",
    "        #     gofinfo = mutual_info_regression(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask])\n",
    "        # else:\n",
    "        #     gofinfo = mutual_info_classif(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask])\n",
    "            \n",
    "        f_values.append((feature_column, f_statistic, p_value))\n",
    "\n",
    "\n",
    "## Convert to DataFrame for easier analysis\n",
    "f_values_df = pd.DataFrame(f_values, columns=['Feature', 'F-value', 'P-value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or analyze the F-values DataFrame\n",
    "sorted_2 = f_values_df['F-value'].argsort()\n",
    "sorted_f_values_df = f_values_df.sort_values(by='F-value')\n",
    "sorted_f_values_df.to_csv(\"ANOVA.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with negative values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_delta1</th>\n",
       "      <th>date_delta2</th>\n",
       "      <th>date_delta3</th>\n",
       "      <th>date_delta4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_delta1, date_delta2, date_delta3, date_delta4]\n",
       "Index: []"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_df[[f'date_delta{i}' for i in range(1,5)]]\n",
    "# Filter rows with negative values\n",
    "negative_rows = df[(df < 0).any(axis=1)]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(\"Rows with negative values:\")\n",
    "negative_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save output file\n",
    "train_df.to_parquet('preprocessed_data.parquet') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
