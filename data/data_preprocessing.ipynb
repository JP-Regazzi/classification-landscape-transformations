{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Mapping\n",
    "CHANGE_TYPE_MAP = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5}\n",
    "CHANGE_STATUS_MAP = {'Greenland': 0.0, 'Land Cleared': 1.0, 'Excavation': 1.0, 'Materials Dumped': 3.0, 'Prior Construction': 3.0, 'Materials Introduced': 4.0, 'Construction Started': 5.0, 'Construction Midway': 6.0, 'Construction Done': 8.0, 'Operational': 10.0, None: None}\n",
    "\n",
    "# Data\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "METRICS = ['std', 'mean']\n",
    "GEOGRAPHY_TYPES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow'] \n",
    "URBAN_TYPES = ['Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial']\n",
    "\n",
    "# Columns groups\n",
    "COLUMNS_TO_DROP = []\n",
    "DATE_COLUMNS = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "CHANGE_STATUS_COLUMNS = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3', 'change_status_date4']\n",
    "CHANGE_STATUS_VALUE_COLUMNS = ['change_status_value_date0', 'change_status_value_date1', 'change_status_value_date2', 'change_status_value_date3', 'change_status_value_date4']\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = 'preprocessed_train.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "train_df_read = gpd.read_file(f'{BASE_PATH}/data/train.geojson', index_col=0)\n",
    "#test_df = gpd.read_file(f'{BASE_PATH}/data/test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_read.copy(deep=True)\n",
    "# Safety\n",
    "MAPPED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean samples with NaN in column  \"change_type\"\n",
    "#df_cleaned = df.dropna(subset=['change_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map change_type\n",
    "if ~MAPPED:\n",
    "    train_df['change_type'] = train_df['change_type'].map(CHANGE_TYPE_MAP)\n",
    "    MAPPED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot encoding\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    train_df[geograph_type] = train_df['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "for urban_type in URBAN_TYPES:\n",
    "    train_df[urban_type] = train_df['urban_type'].apply(lambda x: 1 if urban_type in x else 0)\n",
    "## DROP\n",
    "train_df = train_df.drop(columns=['geography_type', 'urban_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create change_status_values\n",
    "for i in range(len(CHANGE_STATUS_COLUMNS)):\n",
    "    train_df[CHANGE_STATUS_VALUE_COLUMNS[i]] = train_df[CHANGE_STATUS_COLUMNS[i]].map(CHANGE_STATUS_MAP)\n",
    "## DROP\n",
    "train_df = train_df.drop(columns=CHANGE_STATUS_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6973/2799808815.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['area'] = train_df['geometry'].area\n",
      "/tmp/ipykernel_6973/2799808815.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['length'] = train_df['geometry'].length\n",
      "/tmp/ipykernel_6973/2799808815.py:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['centroid_x'] = train_df['geometry'].centroid.x\n",
      "/tmp/ipykernel_6973/2799808815.py:5: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['centroid_y'] = train_df['geometry'].centroid.y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Create new polygon features\n",
    "train_df['area'] = train_df['geometry'].area\n",
    "train_df['length'] = train_df['geometry'].length\n",
    "train_df['centroid_x'] = train_df['geometry'].centroid.x\n",
    "train_df['centroid_y'] = train_df['geometry'].centroid.y\n",
    "bounds = train_df['geometry'].bounds\n",
    "train_df['angle'] = np.arctan((bounds['maxy']-bounds['miny'])/(bounds['maxx']-bounds['minx']))\n",
    "train_df['compactness'] = 4 * np.pi * (train_df['area'] / train_df['length']**2) # the bigger, the closer to square\n",
    "## DROP\n",
    "train_df = train_df.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert date from string to seconds\n",
    "train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(lambda x: pd.to_datetime(x, format='%d-%m-%Y', errors='coerce')).apply(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NaN values have been imputed successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Group the DataFrame by \"change_type\"\n",
    "grouped = train_df.groupby(\"change_type\")\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5,missing_values=np.nan)  # You can adjust the number of neighbors as needed\n",
    "\n",
    "# Initialize an empty list to store the imputed DataFrames\n",
    "imputed_dfs = []\n",
    "\n",
    "# Iterate over each group\n",
    "for change_type, group in grouped:\n",
    "    # Drop the \"change_type\" column before imputation\n",
    "    group_features = group.drop(columns=[\"change_type\"])\n",
    "    \n",
    "    # Impute NaN values within the group\n",
    "    imputed_values = imputer.fit_transform(group_features)\n",
    "    \n",
    "    # Create a DataFrame with imputed values\n",
    "    imputed_df = pd.DataFrame(imputed_values, columns=group_features.columns, index=group_features.index)\n",
    "    \n",
    "    # Concatenate \"change_type\" column back to the imputed DataFrame\n",
    "    imputed_df[\"change_type\"] = change_type\n",
    "    \n",
    "    # Append the imputed DataFrame to the list\n",
    "    imputed_dfs.append(imputed_df)\n",
    "\n",
    "# Concatenate all imputed DataFrames into a single DataFrame\n",
    "imputed_train_df = pd.concat(imputed_dfs)\n",
    "train_df = imputed_train_df.copy()\n",
    "\n",
    "# Verify if there are still NaN values after imputation\n",
    "if train_df.isnull().values.any():\n",
    "    print(\"There are still NaN values remaining after imputation.\")\n",
    "    print(train_df[train_df.isnull().any(axis=1)])\n",
    "else:\n",
    "    print(\"All NaN values have been imputed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIX DATES OUT OF ORDER\n",
    "def sort_dates(row):\n",
    "\n",
    "    # Sort columns by date\n",
    "    columns_order = np.argsort(row[DATE_COLUMNS].values)\n",
    "    new_row = row.copy(deep=True)\n",
    "\n",
    "    # Update date and change_status order\n",
    "    for i in range(5):\n",
    "        new_row[f'date{i}'] = row[f'date{columns_order[i]}']\n",
    "        new_row[f'change_status_value_date{i}'] = row[f'change_status_value_date{columns_order[i]}']\n",
    "\n",
    "    # Update color metrics order\n",
    "    for metric in METRICS:\n",
    "        for color in COLORS:\n",
    "            for i in range(1, 6):\n",
    "                new_row[f'img_{color}_{metric}_date{i}'] = row[f'img_{color}_{metric}_date{columns_order[i-1]+1}']\n",
    "\n",
    "    return new_row\n",
    "\n",
    "train_df = train_df.apply(sort_dates, axis=1)\n",
    "train_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['date0'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create deltas color[std, mean] \n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = train_df[f'img_{color}_{metric}_date{i}'] - train_df[f'img_{color}_{metric}_date{i-1}']\n",
    "            train_df[f'img_{color}_{metric}_delta{i}'] = delta.apply(np.float64)\n",
    "        train_df[f'img_{color}_{metric}_delta_total'] = (train_df[f'img_{color}_{metric}_date5'] - train_df[f'img_{color}_{metric}_date1']).apply(np.float64)\n",
    "\n",
    "## Create deltas time \n",
    "for i in range(1, 5):\n",
    "    train_df[f'date_delta{i}'] = (train_df[f'date{i}'] - train_df[f'date{i-1}']).apply(np.float64)\n",
    "train_df['date_delta_total'] = (train_df[f'date4'] - train_df[f'date0']).apply(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardizing colors mean by the proportion\n",
    "for i in range(1, 6):\n",
    "    color_sum = train_df[f'img_blue_mean_date{i}'] + train_df[f'img_green_mean_date{i}'] + train_df[f'img_red_mean_date{i}']\n",
    "    for color in COLORS:\n",
    "        train_df[f'img_{color}_mean_prop_date{i}'] = train_df[f'img_{color}_mean_date{i}']/color_sum\n",
    "\n",
    "## Create img_{color}_mean_prop_rate\n",
    "num_samples = train_df.shape[0]\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "for color in COLORS:\n",
    "    coef = np.zeros((num_samples))\n",
    "    COLOR_MEAN_COLUMNS = [f'img_{color}_mean_prop_date{i}' for i in range (1,6)]\n",
    "    \n",
    "    Y = np.array(train_df[COLOR_MEAN_COLUMNS].astype(float))\n",
    "    X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "    X = np.dstack((ones,X))\n",
    "    nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "    X[nan_mask,:] = 0\n",
    "    Y[nan_mask] = 0\n",
    "\n",
    "    eye = np.eye(2)*0.0001\n",
    "    for i in range(num_samples):\n",
    "        x = X[i].reshape((5,2))\n",
    "        y = Y[i].reshape((5))\n",
    "        coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "        \n",
    "    train_df[f'img_{color}_mean_prop_rate'] = coef\n",
    "    \n",
    "## Create img_{color}_{metric}_rate\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        coef = np.zeros((num_samples))\n",
    "        COLOR_METRIC_COLUMNS = [f'img_{color}_{metric}_date{i}' for i in range (1,6)]\n",
    "        \n",
    "        Y = np.array(train_df[COLOR_METRIC_COLUMNS].astype(float))\n",
    "        X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "        X = np.dstack((ones,X))\n",
    "        nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "        X[nan_mask,:] = 0\n",
    "        Y[nan_mask] = 0\n",
    "\n",
    "        eye = np.eye(2)*0.0001\n",
    "        for i in range(num_samples):\n",
    "            x = X[i].reshape((5,2))\n",
    "            y = Y[i].reshape((5))\n",
    "            coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "            \n",
    "        train_df[f'img_{color}_{metric}_rate'] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create civilization_rate\n",
    "num_samples = train_df.shape[0]\n",
    "coef = np.zeros((num_samples))\n",
    "time_ctt = 1e9*60*90*24\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "Y = np.array(train_df[CHANGE_STATUS_VALUE_COLUMNS].astype(float))\n",
    "Y_nan_mask = np.isnan(Y)\n",
    "X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]/time_ctt\n",
    "X = np.dstack((ones,X))\n",
    "X[Y_nan_mask,:] = 0\n",
    "Y[Y_nan_mask] = 0\n",
    "\n",
    "eye = np.eye(2)*0.0001\n",
    "for i in range(num_samples):\n",
    "    x = X[i].reshape((5,2))\n",
    "    y = Y[i].reshape((5))\n",
    "    coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "    #print(y, train_df[\"change_type\"].iloc[i])\n",
    "train_df[\"civilizating_rate\"] = coef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalization and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop uncessary columns\n",
    "#train_df = train_df.drop(columns=COLUMNS_TO_DROP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['date_delta1'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANOVA and information gain for each feature\n",
    "from scipy.stats import f_oneway\n",
    "#from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Assuming 'change_type' is the column name for classification effects\n",
    "change_type_column = 'change_type'\n",
    "#CONTINUOUS_FEATURES = ['area_to_length_ratio' ,'area' ,'angle' ,'compactness', 'length']\n",
    "\n",
    "# Group the data by 'change_type'\n",
    "grouped_data = train_df.groupby(change_type_column)\n",
    "\n",
    "# List to store F-values\n",
    "f_values = []\n",
    "#gain_of_info = []\n",
    "\n",
    "# Loop through each feature (column) and calculate F-value\n",
    "for feature_column in train_df.columns:\n",
    "    if feature_column != change_type_column and feature_column != 'geometry':  \n",
    "        # ANOVA\n",
    "        nan_mask = np.isnan(train_df[feature_column])\n",
    "        feature_values = [group[feature_column].dropna() for _, group in grouped_data]\n",
    "        f_statistic, p_value = f_oneway(*feature_values)\n",
    "        # Gain of Info\n",
    "        \n",
    "        \n",
    "        #discrete_bool = feature_column not in CONTINUOUS_FEATURES and 'std' not in feature_column and 'rate' not in feature_column and 'prop' not in feature_column\n",
    "        #gofinfo = mutual_info_classif(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask].astype(str),discrete_features=discrete_bool)\n",
    "        \n",
    "         \n",
    "        #     gofinfo = mutual_info_regression(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask])\n",
    "        # else:\n",
    "        #     gofinfo = mutual_info_classif(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask])\n",
    "            \n",
    "        f_values.append((feature_column, f_statistic, p_value))\n",
    "\n",
    "\n",
    "## Convert to DataFrame for easier analysis\n",
    "f_values_df = pd.DataFrame(f_values, columns=['Feature', 'F-value', 'P-value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or analyze the F-values DataFrame\n",
    "sorted_2 = f_values_df['F-value'].argsort()\n",
    "sorted_f_values_df = f_values_df.sort_values(by='F-value')\n",
    "sorted_f_values_df.to_csv(\"ANOVA.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with negative values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_delta1</th>\n",
       "      <th>date_delta2</th>\n",
       "      <th>date_delta3</th>\n",
       "      <th>date_delta4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_delta1, date_delta2, date_delta3, date_delta4]\n",
       "Index: []"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_df[[f'date_delta{i}' for i in range(1,5)]]\n",
    "# Filter rows with negative values\n",
    "negative_rows = df[(df < 0).any(axis=1)]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(\"Rows with negative values:\")\n",
    "negative_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_red_mean_date1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141.514462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>102.552152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>121.836048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>89.965718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>150.627774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    img_red_mean_date1\n",
       "4           141.514462\n",
       "6           102.552152\n",
       "11          121.836048\n",
       "12           89.965718\n",
       "37          150.627774"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To inspect\n",
    "train_df.head()[[f'img_red_mean_date1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/rick/Dropbox/disciplininhas/SG6/ML/machine-learning-cs/data/data_preprocessing.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rick/Dropbox/disciplininhas/SG6/ML/machine-learning-cs/data/data_preprocessing.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## Save output file\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rick/Dropbox/disciplininhas/SG6/ML/machine-learning-cs/data/data_preprocessing.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_df\u001b[39m.\u001b[39;49mto_parquet(\u001b[39m'\u001b[39;49m\u001b[39mpreprocessed_data.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m) \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3101\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3021\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3022\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3098\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3099\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3101\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   3102\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3103\u001b[0m     path,\n\u001b[1;32m   3104\u001b[0m     engine,\n\u001b[1;32m   3105\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3106\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   3107\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   3108\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3109\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3110\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:476\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(partition_cols, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    475\u001b[0m     partition_cols \u001b[39m=\u001b[39m [partition_cols]\n\u001b[0;32m--> 476\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    478\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[1;32m    480\u001b[0m impl\u001b[39m.\u001b[39mwrite(\n\u001b[1;32m    481\u001b[0m     df,\n\u001b[1;32m    482\u001b[0m     path_or_buf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:67\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     65\u001b[0m             error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(err)\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to find a usable engine; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtried using: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfastparquet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA suitable version of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupport.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to import the above resulted in these errors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00merror_msgs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "## Save output file\n",
    "train_df.to_parquet('preprocessed_data.parquet') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
