{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Mapping\n",
    "CHANGE_TYPE_MAP = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5}\n",
    "CHANGE_STATUS_MAP = {'Greenland': 0.0, 'Land Cleared': 1.0, 'Excavation': 1.0, 'Materials Dumped': 3.0, 'Prior Construction': 3.0, 'Materials Introduced': 4.0, 'Construction Started': 5.0, 'Construction Midway': 6.0, 'Construction Done': 8.0, 'Operational': 10.0, None: None}\n",
    "\n",
    "# Data\n",
    "COLORS = ['red', 'green', 'blue']\n",
    "METRICS = ['std', 'mean']\n",
    "GEOGRAPHY_TYPES = ['Dense Forest', 'Grass Land', 'Sparse Forest', 'Farms', 'River',\n",
    "                   'Coastal', 'Lakes', 'Barren Land', 'Desert', 'Hills', 'Snow'] \n",
    "URBAN_TYPES = ['Sparse Urban', 'Rural', 'Dense Urban', 'Urban Slum', 'Industrial']\n",
    "\n",
    "# Columns groups\n",
    "COLUMNS_TO_DROP = []\n",
    "DATE_COLUMNS = ['date0', 'date1', 'date2', 'date3', 'date4']\n",
    "CHANGE_STATUS_COLUMNS = ['change_status_date0', 'change_status_date1', 'change_status_date2', 'change_status_date3', 'change_status_date4']\n",
    "CHANGE_STATUS_VALUE_COLUMNS = ['change_status_value_date0', 'change_status_value_date1', 'change_status_value_date2', 'change_status_value_date3', 'change_status_value_date4']\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = 'preprocessed_train.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "train_df_read = gpd.read_file(f'{BASE_PATH}/data/train.geojson', index_col=0)\n",
    "#test_df = gpd.read_file(f'{BASE_PATH}/data/test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_read.copy(deep=True)\n",
    "# Safety\n",
    "MAPPED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot encoding\n",
    "for geograph_type in GEOGRAPHY_TYPES:\n",
    "    train_df[geograph_type] = train_df['geography_type'].apply(lambda x: 1 if geograph_type in x else 0)\n",
    "for urban_type in URBAN_TYPES:\n",
    "    train_df[urban_type] = train_df['urban_type'].apply(lambda x: 1 if urban_type in x else 0)\n",
    "## DROP\n",
    "train_df = train_df.drop(columns=['geography_type', 'urban_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create change_status_values\n",
    "for i in range(len(CHANGE_STATUS_COLUMNS)):\n",
    "    train_df[CHANGE_STATUS_VALUE_COLUMNS[i]] = train_df[CHANGE_STATUS_COLUMNS[i]].map(CHANGE_STATUS_MAP)\n",
    "## DROP\n",
    "train_df = train_df.drop(columns=CHANGE_STATUS_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6973/2799808815.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['area'] = train_df['geometry'].area\n",
      "/tmp/ipykernel_6973/2799808815.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['length'] = train_df['geometry'].length\n",
      "/tmp/ipykernel_6973/2799808815.py:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['centroid_x'] = train_df['geometry'].centroid.x\n",
      "/tmp/ipykernel_6973/2799808815.py:5: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  train_df['centroid_y'] = train_df['geometry'].centroid.y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Create new polygon features\n",
    "train_df['area'] = train_df['geometry'].area\n",
    "train_df['length'] = train_df['geometry'].length\n",
    "train_df['centroid_x'] = train_df['geometry'].centroid.x\n",
    "train_df['centroid_y'] = train_df['geometry'].centroid.y\n",
    "bounds = train_df['geometry'].bounds\n",
    "train_df['angle'] = np.arctan((bounds['maxy']-bounds['miny'])/(bounds['maxx']-bounds['minx']))\n",
    "train_df['compactness'] = 4 * np.pi * (train_df['area'] / train_df['length']**2) # the bigger, the closer to square\n",
    "## DROP\n",
    "train_df = train_df.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert date from string to seconds\n",
    "train_df[DATE_COLUMNS] = train_df[DATE_COLUMNS].apply(lambda x: pd.to_datetime(x, format='%d-%m-%Y', errors='coerce')).apply(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NaN values have been imputed successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Group the DataFrame by \"change_type\"\n",
    "grouped = train_df.groupby(\"change_type\")\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5,missing_values=np.nan)  # You can adjust the number of neighbors as needed\n",
    "\n",
    "# Initialize an empty list to store the imputed DataFrames\n",
    "imputed_dfs = []\n",
    "\n",
    "# Iterate over each group\n",
    "for change_type, group in grouped:\n",
    "    # Drop the \"change_type\" column before imputation\n",
    "    group_features = group.drop(columns=[\"change_type\"])\n",
    "    \n",
    "    # Impute NaN values within the group\n",
    "    imputed_values = imputer.fit_transform(group_features)\n",
    "    \n",
    "    # Create a DataFrame with imputed values\n",
    "    imputed_df = pd.DataFrame(imputed_values, columns=group_features.columns, index=group_features.index)\n",
    "    \n",
    "    # Concatenate \"change_type\" column back to the imputed DataFrame\n",
    "    imputed_df[\"change_type\"] = change_type\n",
    "    \n",
    "    # Append the imputed DataFrame to the list\n",
    "    imputed_dfs.append(imputed_df)\n",
    "\n",
    "# Concatenate all imputed DataFrames into a single DataFrame\n",
    "imputed_train_df = pd.concat(imputed_dfs)\n",
    "\n",
    "# Verify if there are still NaN values after imputation\n",
    "if imputed_train_df.isnull().values.any():\n",
    "    print(\"There are still NaN values remaining after imputation.\")\n",
    "    print(imputed_train_df[imputed_train_df.isnull().any(axis=1)])\n",
    "else:\n",
    "    print(\"All NaN values have been imputed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FIX DATES OUT OF ORDER\n",
    "# def sort_dates(row):\n",
    "\n",
    "#     # Sort columns by date\n",
    "#     columns_order = np.argsort(row[DATE_COLUMNS].values)\n",
    "#     new_row = row.copy(deep=True)\n",
    "\n",
    "#     # Update date and change_status order\n",
    "#     for i in range(5):\n",
    "#         new_row[f'date{i}'] = row[f'date{columns_order[i]}']\n",
    "#         new_row[f'change_status_date{i}'] = row[f'change_status_date{columns_order[i]}']\n",
    "\n",
    "#     # Update color metrics order\n",
    "#     for metric in METRICS:\n",
    "#         for color in COLORS:\n",
    "#             for i in range(1, 6):\n",
    "#                 new_row[f'img_{color}_{metric}_date{i}'] = row[f'img_{color}_{metric}_date{columns_order[i-1]+1}']\n",
    "    \n",
    "\n",
    "#     return new_row\n",
    "\n",
    "# train_df = train_df.apply(sort_dates, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create deltas color[std, mean] \n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        for i in range(2, 6):\n",
    "            delta = train_df[f'img_{color}_{metric}_date{i}'] - train_df[f'img_{color}_{metric}_date{i-1}']\n",
    "            train_df[f'img_{color}_{metric}_delta{i}'] = delta.apply(np.float64)\n",
    "        train_df[f'img_{color}_{metric}_delta_total'] = (train_df[f'img_{color}_{metric}_date5'] - train_df[f'img_{color}_{metric}_date1']).apply(np.float64)\n",
    "\n",
    "## Create deltas time \n",
    "for i in range(1, 5):\n",
    "    train_df[f'date_delta{i}'] = (train_df[f'date{i}'] - train_df[f'date{i-1}']).dt.total_seconds()\n",
    "train_df['date_delta_total'] = (train_df[f'date4'] - train_df[f'date0']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardizing colors mean by the proportion\n",
    "for i in range(1, 6):\n",
    "    color_sum = train_df[f'img_blue_mean_date{i}'] + train_df[f'img_green_mean_date{i}'] + train_df[f'img_red_mean_date{i}']\n",
    "    for color in COLORS:\n",
    "        train_df[f'img_{color}_mean_prop_date{i}'] = train_df[f'img_{color}_mean_date{i}']/color_sum\n",
    "\n",
    "## Create img_{color}_mean_prop_rate\n",
    "num_samples = train_df.shape[0]\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "for color in COLORS:\n",
    "    coef = np.zeros((num_samples))\n",
    "    COLOR_MEAN_COLUMNS = [f'img_{color}_mean_prop_date{i}' for i in range (1,6)]\n",
    "    \n",
    "    Y = np.array(train_df[COLOR_MEAN_COLUMNS].astype(float))\n",
    "    X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "    X = np.dstack((ones,X))\n",
    "    nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "    X[nan_mask,:] = 0\n",
    "    Y[nan_mask] = 0\n",
    "\n",
    "    eye = np.eye(2)*0.0001\n",
    "    for i in range(num_samples):\n",
    "        x = X[i].reshape((5,2))\n",
    "        y = Y[i].reshape((5))\n",
    "        coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "        \n",
    "    train_df[f'img_{color}_mean_prop_rate'] = coef\n",
    "    \n",
    "## Create img_{color}_{metric}_rate\n",
    "for metric in METRICS:\n",
    "    for color in COLORS:\n",
    "        coef = np.zeros((num_samples))\n",
    "        COLOR_METRIC_COLUMNS = [f'img_{color}_{metric}_date{i}' for i in range (1,6)]\n",
    "        \n",
    "        Y = np.array(train_df[COLOR_METRIC_COLUMNS].astype(float))\n",
    "        X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]\n",
    "        X = np.dstack((ones,X))\n",
    "        nan_mask = np.isnan(Y) | np.isnan(X[:,:,1])\n",
    "        X[nan_mask,:] = 0\n",
    "        Y[nan_mask] = 0\n",
    "\n",
    "        eye = np.eye(2)*0.0001\n",
    "        for i in range(num_samples):\n",
    "            x = X[i].reshape((5,2))\n",
    "            y = Y[i].reshape((5))\n",
    "            coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "            \n",
    "        train_df[f'img_{color}_{metric}_rate'] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map change_type\n",
    "if ~MAPPED:\n",
    "    train_df['change_type'] = train_df['change_type'].map(CHANGE_TYPE_MAP)\n",
    "    MAPPED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.10/site-packages/geopandas/geodataframe.py:1525: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "/home/rick/.local/lib/python3.10/site-packages/geopandas/geodataframe.py:1525: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "## Create civilization_rate\n",
    "num_samples = train_df.shape[0]\n",
    "coef = np.zeros((num_samples))\n",
    "time_ctt = 1e9*60*90*24\n",
    "ones = np.ones((num_samples,5,1))\n",
    "\n",
    "Y = np.array(train_df[CHANGE_STATUS_VALUE_COLUMNS].astype(float))\n",
    "Y_nan_mask = np.isnan(Y)\n",
    "X = np.array(train_df[DATE_COLUMNS].apply(np.float64))[:,:,np.newaxis]/time_ctt\n",
    "X = np.dstack((ones,X))\n",
    "X[Y_nan_mask,:] = 0\n",
    "Y[Y_nan_mask] = 0\n",
    "\n",
    "eye = np.eye(2)*0.0001\n",
    "for i in range(num_samples):\n",
    "    x = X[i].reshape((5,2))\n",
    "    y = Y[i].reshape((5))\n",
    "    coef[i] = (np.linalg.inv(eye+x.T@x)@x.T@y)[1]\n",
    "    #print(y, train_df[\"change_type\"].iloc[i])\n",
    "train_df[\"civilizating_rate\"] = coef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalization and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop uncessary columns\n",
    "#train_df = train_df.drop(columns=COLUMNS_TO_DROP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['index'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change_status_value_date1 True\n",
      "change_status_value_date2 True\n",
      "change_status_value_date3 True\n",
      "change_status_value_date4 True\n",
      "civilizating_rate False\n"
     ]
    }
   ],
   "source": [
    "## ANOVA and information gain for each feature\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Assuming 'change_type' is the column name for classification effects\n",
    "change_type_column = 'change_type'\n",
    "CONTINUOUS_FEATURES = ['area_to_length_ratio' ,'area' ,'angle' ,'compactness', 'length']\n",
    "\n",
    "# Group the data by 'change_type'\n",
    "grouped_data = train_df.groupby(change_type_column)\n",
    "\n",
    "# List to store F-values\n",
    "f_values = []\n",
    "gain_of_info = []\n",
    "\n",
    "# Loop through each feature (column) and calculate F-value\n",
    "for feature_column in train_df.columns:\n",
    "    if feature_column != change_type_column and feature_column != 'geometry':  \n",
    "        # ANOVA\n",
    "        feature_values = [group[feature_column].dropna() for _, group in grouped_data]\n",
    "        f_statistic, p_value = f_oneway(*feature_values)\n",
    "        # Gain of Info\n",
    "        nan_mask = np.isnan(train_df[feature_column])\n",
    "        discrete_bool = feature_column not in CONTINUOUS_FEATURES and 'std' not in feature_column and 'rate' not in feature_column and 'prop' not in feature_column\n",
    "        gofinfo = mutual_info_classif(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask].astype(str),discrete_features=discrete_bool)\n",
    "        \n",
    "         \n",
    "        #     gofinfo = mutual_info_regression(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask])\n",
    "        # else:\n",
    "        #     gofinfo = mutual_info_classif(train_df[feature_column][~nan_mask].values.reshape(-1, 1), train_df[change_type_column][~nan_mask])\n",
    "            \n",
    "        f_values.append((feature_column, f_statistic, p_value, gofinfo))\n",
    "\n",
    "\n",
    "## Convert to DataFrame for easier analysis\n",
    "f_values_df = pd.DataFrame(f_values, columns=['Feature', 'F-value', 'P-value', 'Gain of Info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or analyze the F-values DataFrame\n",
    "sorted_1 = f_values_df['Gain of Info'].argsort()\n",
    "sorted_2 = f_values_df['F-value'].argsort()\n",
    "sorted_score = (sorted_1 + 3*sorted_2)/4\n",
    "f_values_df['Sorted Score'] = sorted_score\n",
    "sorted_f_values_df = f_values_df.sort_values(by='F-value')\n",
    "sorted_f_values_df.to_csv(\"ANOVA.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To inspect\n",
    "train_df.head()[[f'date_delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df[[f'date{i}' for i in range(0,5)]]\n",
    "# Filter rows with negative values\n",
    "negative_rows = df#df[(df < 0).any(axis=1)]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(\"Rows with negative values:\")\n",
    "for row in negative_rows.values:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save output file\n",
    "train_df.to_file(OUTPUT_FILE) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
